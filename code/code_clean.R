# Load Required Libraries --------------------------------
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(gglasso)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC)
library(car)
# Set Working Directory ---------------------------------
data_path <- "C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx"

# Import Data -------------------------------------------
data <- read_excel(data_path)

# Data Preprocessing ------------------------------------

## Remove ID Variables
id_vars <- c("Código de Crédito", "ID Cliente", "No Pagaré Rotativo")
data <- data[, !names(data) %in% id_vars]

## Remove Single-Value Columns
single_value_vars <- c("Clasificación Tipo Crédito")
data <- data[, !names(data) %in% single_value_vars]

## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
                    "income_group", "city_born", "max_education", "gender", 
                    "contributions_balance", "credit_limit", "capital_balance",
                    "capital_due30", "days_due", "date_approval",
                    "installment", "periodicity", "credit_duration", "date_limit",
                    "dtf_approval_date", "fx_approval_date", "default_90")
if (length(friendly_names) == ncol(data)) {
  colnames(data) <- friendly_names
} else {
  stop("Column name mismatch.")
}

## Handle Missing Values
na_counts <- colSums(is.na(data))
print(na_counts)

# Transformations ---------------------------------------

## Filter Credit Limit > 50,000
data <- data[data$credit_limit > 50000, ]

## Map Periodicity to Numeric
data <- data %>%
  mutate(periodicity_num = case_when(
    periodicity == "Mensual" ~ 30,
    periodicity == "Bimensual" ~ 60,
    periodicity == "Quincenal" ~ 15,
    TRUE ~ NA_real_
  )) %>%
  select(-periodicity)

## Map Education Levels to Numeric
# If we remove this then the model does not find min lambda
# data <- data %>%
#   mutate(max_education = case_when(
#     max_education == "primaria" ~ 1,
#     max_education == "secundaria" ~ 2,
#     max_education == "técnico" ~ 3,
#     max_education == "tecnólogo" ~ 4,
#     max_education == "Universitario" ~ 5,
#     max_education == "Posgrado" ~ 6,
#     TRUE ~ NA_real_
#   ))



## Create Derived Variables
data <- data %>%
  mutate(
    installment_periodic = installment / periodicity_num,
    time_difference_days = as.numeric(difftime(as.Date(date_limit), as.Date(date_approval), units = "days"))
  )

## Date furhter information
# First create year, month, day, weekday

# Extract features from the date-time variables
# date approval
data$m_date_approval <- format(data$date_approval, "%m")
data$wd_date_approval <- weekdays(data$date_approval)

# date limit
data$m_date_limit <- format(data$date_limit, "%m")
data$wd_date_limit <- weekdays(data$date_limit)


## Convert POSIXct to numeric
## Identify columns of type POSIXct
posix_columns <- sapply(data, function(col) inherits(col, "POSIXct"))

## Convert POSIXct columns to numeric
data[posix_columns] <- lapply(data[posix_columns], as.numeric)



## Convert Characters to Factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)



## Convert all numeric to log
# Identify numeric columns in the dataframe
numeric_columns <- sapply(data, is.numeric)

# Exclude the specified columns
exclude_columns <- c("default_90", "dtf_apporval_date")
columns_to_transform <- setdiff(names(data)[numeric_columns], exclude_columns)

# Apply the natural logarithm to the selected columns, adding 1 to handle zeros
data[columns_to_transform] <- lapply(data[columns_to_transform], function(col) log(col + 1))

# Move "default_90" to the last column
data <- data[, c(setdiff(names(data), "default_90"), "default_90")]



# Train-Test Split --------------------------------------
set.seed(123)
split <- sample.split(data$default_90, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)



# Exploratory Data Analysis -----------------------------

## Numeric Variables------------
numeric_data <- train_data %>% select(where(is.numeric))
skim(numeric_data)
### Histograms
for (col_name in colnames(numeric_data)) {
  hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
       xlab = col_name, col = "lightblue", border = "black")
}

### Density Plots--------------
for (col_name in colnames(numeric_data)) {
  print(ggplot(train_data, aes(x = .data[[col_name]], fill = factor(default_90))) +
    geom_density(alpha = 0.5) +
    labs(title = paste("Density Plot of", col_name), x = col_name, fill = "Target") +
    theme_minimal())
}


## Non-Numeric Variables------------
non_numeric_data <- train_data %>% select(where(~ !is.numeric(.)))
unique_counts <- sapply(non_numeric_data, function(x) length(unique(x)))
mode_values <- sapply(non_numeric_data, function(x) names(which.max(table(x))))
print(mode_values)

## Correlation Analysis ----------------------------------

cor_matrix <- cor(numeric_data, use = "complete.obs")
melted_cor_matrix <- melt(cor_matrix)
ggplot(data = melted_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  coord_fixed() + theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Model Training and Evaluation ------------------------

## Logistic Regression----------------
logistic_model <- glm(default_90 ~ ., data = train_data, family = binomial)

### Evaluate Logistic Regression------------
evaluate_model <- function(model, test_data, target_col) {
  predicted_prob <- predict(model, newdata = test_data, type = "response")
  predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
  accuracy <- mean(predicted_class == test_data[[target_col]])
  confusion <- confusionMatrix(factor(predicted_class), factor(test_data[[target_col]]))
  f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
    (confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
  return(list(accuracy = accuracy, f1_score = f1_score))
}

logistic_results <- evaluate_model(logistic_model, test_data, "default_90")
print(logistic_results)


### Post-Estimation Plots---------------
par(mfrow = c(2,2))
plot(logistic_model)

#### 1. ROC Curve and AUC ------------------------------------
par(mfrow = c(1,1))

# Predicted probabilities
predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")

# ROC and AUC
roc_curve <- roc(test_data$default_90, predicted_prob)
auc_value <- auc(roc_curve)

# Plot ROC Curve
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")

#### 2. Confusion Matrix Heatmap -----------------------------

# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)

# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)

# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  geom_text(aes(label = Freq), color = "black") +
  labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
  theme_minimal()

#### 3. Coefficient Plot -------------------------------------
# Extract coefficients
coefficients <- summary(logistic_model)$coefficients
coef_data <- as.data.frame(coefficients)
coef_data$Variable <- rownames(coefficients)
rownames(coef_data) <- NULL

# Plot coefficients
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  coord_flip() +
  labs(title = "Coefficient Plot", x = "Variable", y = "Estimate") +
  theme_minimal()

#### 4. Calibration Curve ------------------------------------

# Bin predicted probabilities
calibration_data <- data.frame(
  Predicted = predicted_prob,
  Observed = test_data$default_90
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)

# Mean predicted and observed probabilities by bin
calibration_curve <- calibration_data %>%
  group_by(Bin) %>%
  summarize(
    Mean_Predicted = mean(Predicted),
    Mean_Observed = mean(Observed)
  )

# Plot calibration curve
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
  theme_minimal()


## Lasso Logistic Regression--------------

### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
x_train <- as.matrix(train_data %>% select(-default_90))
y_train <- train_data$default_90

x_test <- as.matrix(test_data %>% select(-default_90))
y_test <- test_data$default_90

# scale predictors
x_train <- scale(x_train)
x_test <- scale(x_test)


### 2. Perform Cross-Validation for Lasso Logistic Regression --
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1, nfolds = 10)

# Plot of Cross-Validation Results 
# Optimal lambda
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se

# Plot deviance against log(lambda)
par(mfrow= c(1,1))
plot(cv_lasso, main = "Cross-Validation for Lasso Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)

# We find the lambda to the top left, meaning it could be further left:
# Define a custom lambda sequence (smaller values)
# lambda_grid <- 10^seq(-10, 2, length = 100)  # Adjust range to include smaller lambda values
# 
# # Perform Cross-Validation with custom lambda grid
# set.seed(123)
# cv_lasso <- cv.glmnet(
#   x_train, y_train, 
#   family = "binomial", 
#   alpha = 1, 
#   lambda = lambda_grid, 
#   nfolds = 10
# )
# 
# # Optimal lambdas
# lambda_min <- cv_lasso$lambda.min
# lambda_1se <- cv_lasso$lambda.1se
# 
# # Plot the updated deviance vs log(lambda) with the extended search range
# plot(cv_lasso, main = "Extended Cross-Validation for Lasso Logistic Regression")
# abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
# abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
# legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
# 
# 
# # Lambda is still max left, there is something off with the model

# Optimal lambda
lambda_optimal <- cv_lasso$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")


### 3. Fit the Final Model with Optimal Lambda -----------------
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = lambda_optimal)

### 4. Evaluate Model ------------------------------------------
# Predicted probabilities
predicted_prob <- predict(lasso_model, s = lambda_optimal, newx = x_test, type = "response")

# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)

# Model evaluation metrics
evaluate_lasso <- function(predicted_class, predicted_prob, y_test) {
  accuracy <- mean(predicted_class == y_test)
  confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
  auc_value <- auc(roc(y_test, predicted_prob))
  f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
    (confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
  return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}

lasso_results <- evaluate_lasso(predicted_class, predicted_prob, y_test)
print(lasso_results)

# 5. Post-Estimation Plots -----------------------------------

# (1) ROC Curve and AUC
roc_curve <- roc(y_test, predicted_prob)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(lasso_results$auc, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")

# (2) Confusion Matrix Heatmap
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  geom_text(aes(label = Freq), color = "black") +
  labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
  theme_minimal()

# (3) Coefficient Plot
coef_data <- as.data.frame(as.matrix(coef(lasso_model, s = lambda_optimal)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")

ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  coord_flip() +
  labs(title = "Lasso Coefficient Plot", x = "Variable", y = "Coefficient") +
  theme_minimal()

# (4) Calibration Curve
calibration_data <- data.frame(
  Predicted = as.vector(predicted_prob),
  Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)

calibration_curve <- calibration_data %>%
  group_by(Bin) %>%
  summarize(
    Mean_Predicted = mean(Predicted),
    Mean_Observed = mean(Observed)
  )

ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
  theme_minimal()



## Elastic Net Logistic Regression--------------

### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
x_train <- as.matrix(train_data %>% select(-default_90))
y_train <- train_data$default_90

x_test <- as.matrix(test_data %>% select(-default_90))
y_test <- test_data$default_90

# Scale predictors
x_train <- scale(x_train)
x_test <- scale(x_test)

### 2. Perform Cross-Validation for Elastic Net Logistic Regression --
# Elastic net uses `alpha` to mix lasso (alpha = 1) and ridge (alpha = 0)
set.seed(123)
cv_elastic_net <- cv.glmnet(
  x_train, y_train, 
  family = "binomial", 
  alpha = 0.5,        # Elastic net (mix of lasso and ridge)
  nfolds = 10         # 10-fold cross-validation
)

# Plot of Cross-Validation Results
# Optimal lambdas
lambda_min <- cv_elastic_net$lambda.min
lambda_1se <- cv_elastic_net$lambda.1se

# Plot deviance against log(lambda)
par(mfrow = c(1, 1))
plot(cv_elastic_net, main = "Cross-Validation for Elastic Net Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)

cat("Optimal lambda (min):", lambda_min, "\n")
cat("Optimal lambda (1se):", lambda_1se, "\n")


# We find the lambda to the top left, meaning it could be further left:
# Define a custom lambda sequence (smaller values)
lambda_grid <- 10^seq(-10, 2, length = 100)  # Adjust range to include smaller lambda values

# Perform Cross-Validation with custom lambda grid
set.seed(123)
cv_elastic_net <- cv.glmnet(
  x_train, y_train,
  family = "binomial",
  alpha = 0.5,
  lambda = lambda_grid,
  nfolds = 10
)


# Optimal lambdas
lambda_min <- cv_elastic_net$lambda.min
lambda_1se <- cv_elastic_net$lambda.1se

# Plot the updated deviance vs log(lambda) with the extended search range
plot(cv_elastic_net, main = "Extended Cross-Validation for Elastic Net Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)




### 3. Fit the Final Model with Optimal Lambda -----------------
elastic_net_model <- glmnet(
  x_train, y_train, 
  family = "binomial", 
  alpha = 0.5,        # Elastic net
  lambda = lambda_min # Use lambda.min for final model
)

### 4. Evaluate Model ------------------------------------------
# Predicted probabilities
predicted_prob <- predict(elastic_net_model, s = lambda_min, newx = x_test, type = "response")

# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)

# Model evaluation metrics
evaluate_elastic_net <- function(predicted_class, predicted_prob, y_test) {
  accuracy <- mean(predicted_class == y_test)
  confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
  auc_value <- auc(roc(y_test, predicted_prob))
  f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
    (confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
  return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}

elastic_net_results <- evaluate_elastic_net(predicted_class, predicted_prob, y_test)
print(elastic_net_results)

### 5. Post-Estimation Plots -----------------------------------

# (1) ROC Curve and AUC
roc_curve <- roc(y_test, predicted_prob)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(elastic_net_results$auc, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")

# (2) Confusion Matrix Heatmap
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "blue") +
  geom_text(aes(label = Freq), color = "black") +
  labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
  theme_minimal()

# (3) Coefficient Plot
coef_data <- as.data.frame(as.matrix(coef(elastic_net_model, s = lambda_min)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")

ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  coord_flip() +
  labs(title = "Elastic Net Coefficient Plot", x = "Variable", y = "Coefficient") +
  theme_minimal()

# (4) Calibration Curve
calibration_data <- data.frame(
  Predicted = as.vector(predicted_prob),
  Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)

calibration_curve <- calibration_data %>%
  group_by(Bin) %>%
  summarize(
    Mean_Predicted = mean(Predicted),
    Mean_Observed = mean(Observed)
  )

ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
  theme_minimal()



# Add  Group Lasso similarly in modular functions
