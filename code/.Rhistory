ggplot(explained_variance_df, aes(x = Components, y = CumulativeVariance)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
labs(title = "Cumulative Variance Explained by Components", x = "Number of Components", y = "Cumulative Variance")
# plot and choose graphically
components_90 <- which(explained_variance >= 0.90)[1]  # Select components that explain at least 90% of variance
cat("Number of components explaining at least 90% of variance:", components_90, "\n")
### Use Only Top Components -------------------------------------
# Fit sparse PCA again with the optimal number of components
fit <- spca(X_train, k = components_90, alpha = 0.1, scale = TRUE, verbose = FALSE)
# Extract Principal Components
V <- fit$loadings
pc_train <- as.data.frame(x_train %*% V)
V
pc_train <- as.data.frame(x_train %*% V)
# LAB CODE
### Step 1: Initial Sparse PCA Fit --------------------------------
k <- 10  # Number of components to consider
set.seed(123)
fit <- spca(x_train, k = k, scale = TRUE, verbose = FALSE)
x_train
X_train
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
x_train
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
### Apply Sparse PCA to Train Data ------------------------------
set.seed(123)
# Center the data and check the scale parametrization-
k <- 10  # Number of components to consider initially
fit <- spca(X_train, k = k, scale = TRUE, verbose = FALSE)
X_train
colnames(X_train)
X_train
# Create x_train_spca as matrix without column default 90 (train_data_no_target)
x_train_spca <- as.matrix(train_data_no_target)
# Create x_train_spca as matrix without column default 90 (train_data_no_target)
x_train_spca <- as.matrix(train_data_no_target)
### Apply Sparse PCA to Train Data ------------------------------
set.seed(123)
# Center the data and check the scale parametrization-
k <- 10  # Number of components to consider initially
fit <- spca(x_train_spca, k = k, scale = TRUE, verbose = FALSE)
x_train_spca
# Create x_train_spca as matrix without column default 90 as in group lasso
x_train_spca <- X_train
### Apply Sparse PCA to Train Data ------------------------------
set.seed(123)
# Center the data and check the scale parametrization-
k <- 10  # Number of components to consider initially
fit <- spca(x_train_spca, k = k, scale = TRUE, verbose = FALSE)
### Calculate Cumulative Variance Explained --------------------
explained_variance <- cumsum(fit$sdev^2) / sum(fit$sdev^2)
components <- 1:length(explained_variance)
explained_variance_df <- data.frame(Components = components, CumulativeVariance = explained_variance)
ggplot(explained_variance_df, aes(x = Components, y = CumulativeVariance)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
labs(title = "Cumulative Variance Explained by Components", x = "Number of Components", y = "Cumulative Variance")
dim(X_train)
# Assuming `x_train_spca` is your feature matrix with 105 columns
X <- x_train_spca  # This is your original data matrix with 105 features
# Step 1: Fit Sparse PCA with `k = 10`
set.seed(123)
k <- 10  # Number of components to consider
fit <- spca(X, k = k, scale = TRUE, verbose = FALSE)
# Step 2: Calculate Explained Variance for the first `k` components
explained_variance_k <- cumsum(fit$sdev^2) / sum(fit$sdev^2)
explained_variance_k_10 <- explained_variance_k[length(explained_variance_k)]  # Explained variance by 10 components
# Step 3: Fit Sparse PCA with `k = 105` (to get the full variance)
fit_full <- spca(X, k = 105, scale = TRUE, verbose = FALSE)
# Step 4: Calculate Total Variance for all `105` components
explained_variance_full <- cumsum(fit_full$sdev^2) / sum(fit_full$sdev^2)
explained_variance_total <- explained_variance_full[length(explained_variance_full)]  # Total explained variance
# Step 5: Calculate Ratio of Explained Variance for k=10 vs Full (105 components)
variance_ratio <- explained_variance_k_10 / explained_variance_total
cat("Variance explained by the first 10 components as a ratio of the total variance:", variance_ratio, "\n")
# Optional: Plot Cumulative Variance Explained by k Components
components <- 1:length(explained_variance_k)
explained_variance_df <- data.frame(Components = components, CumulativeVariance = explained_variance_k)
ggplot(explained_variance_df, aes(x = Components, y = CumulativeVariance)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
labs(title = "Cumulative Variance Explained by First 10 Components", x = "Number of Components", y = "Cumulative Variance")
# Step 5: Calculate Ratio of Explained Variance for k=10 vs Full (105 components)
variance_ratio <- explained_variance_k_10 / explained_variance_total
cat("Variance explained by the first 10 components as a ratio of the total variance:", variance_ratio, "\n")
# Step 4: Calculate Total Variance for all `105` components
explained_variance_full <- cumsum(fit_full$sdev^2) / sum(fit_full$sdev^2)
explained_variance_total <- explained_variance_full[length(explained_variance_full)]  # Total explained variance
# Step 5: Calculate Ratio of Explained Variance for k=10 vs Full (105 components)
variance_ratio <- explained_variance_k_10 / explained_variance_total
cat("Variance explained by the first 10 components as a ratio of the total variance:", variance_ratio, "\n")
# Optional: Plot Cumulative Variance Explained by k Components
components <- 1:length(explained_variance_k)
explained_variance_df <- data.frame(Components = components, CumulativeVariance = explained_variance_k)
ggplot(explained_variance_df, aes(x = Components, y = CumulativeVariance)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
labs(title = "Cumulative Variance Explained by First 10 Components", x = "Number of Components", y = "Cumulative Variance")
k <- 3  # Number of components to consider
fit <- spca(X, k = k, scale = TRUE, verbose = FALSE)
# Step 2: Calculate Explained Variance for the first `k` components
explained_variance_k <- cumsum(fit$sdev^2) / sum(fit$sdev^2)
explained_variance_k_10 <- explained_variance_k[length(explained_variance_k)]  # Explained variance by 10 components
# Step 3: Fit Sparse PCA with `k = 105` (to get the full variance)
fit_full <- spca(X, k = 105, scale = TRUE, verbose = FALSE)
# Step 4: Calculate Total Variance for all `105` components
explained_variance_full <- cumsum(fit_full$sdev^2) / sum(fit_full$sdev^2)
explained_variance_total <- explained_variance_full[length(explained_variance_full)]  # Total explained variance
# Step 5: Calculate Ratio of Explained Variance for k=10 vs Full (105 components)
variance_ratio <- explained_variance_k_10 / explained_variance_total
cat("Variance explained by the first 10 components as a ratio of the total variance:", variance_ratio, "\n")
# Optional: Plot Cumulative Variance Explained by k Components
components <- 1:length(explained_variance_k)
explained_variance_df <- data.frame(Components = components, CumulativeVariance = explained_variance_k)
ggplot(explained_variance_df, aes(x = Components, y = CumulativeVariance)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
labs(title = "Cumulative Variance Explained by First 10 Components", x = "Number of Components", y = "Cumulative Variance")
# plot and choose graphically
components_90 <- which(explained_variance >= 0.90)[1]  # Select components that explain at least 90% of variance
# Assuming `x_train_spca` is your feature matrix with 105 columns
X <- x_train_spca  # This is your original data matrix with 105 features
# Step 1: Fit Sparse PCA with `k = 10`
set.seed(123)
k <- 2  # Number of components to consider
fit <- spca(X, k = k, scale = TRUE, verbose = FALSE)
# Step 2: Calculate Explained Variance for the first `k` components
explained_variance_k <- cumsum(fit$sdev^2) / sum(fit$sdev^2)
explained_variance_k_10 <- explained_variance_k[length(explained_variance_k)]  # Explained variance by 10 components
# Step 3: Fit Sparse PCA with `k = 105` (to get the full variance)
fit_full <- spca(X, k = 105, scale = TRUE, verbose = FALSE)
# Step 4: Calculate Total Variance for all `105` components
explained_variance_full <- cumsum(fit_full$sdev^2) / sum(fit_full$sdev^2)
explained_variance_total <- explained_variance_full[length(explained_variance_full)]  # Total explained variance
# Step 5: Calculate Ratio of Explained Variance for k=10 vs Full (105 components)
variance_ratio <- explained_variance_k_10 / explained_variance_total
cat("Variance explained by the first 10 components as a ratio of the total variance:", variance_ratio, "\n")
# Optional: Plot Cumulative Variance Explained by k Components
components <- 1:length(explained_variance_k)
explained_variance_df <- data.frame(Components = components, CumulativeVariance = explained_variance_k)
ggplot(explained_variance_df, aes(x = Components, y = CumulativeVariance)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 0.90, linetype = "dashed", color = "red") +
labs(title = "Cumulative Variance Explained by First 10 Components", x = "Number of Components", y = "Cumulative Variance")
# LAB CODE
### Step 1: Initial Sparse PCA Fit --------------------------------
k <- 10  # Number of components to consider
set.seed(123)
fit <- spca(x_train_pca, k = k, scale = TRUE, verbose = FALSE)
summary(fit)
# LAB CODE
### Step 1: Initial Sparse PCA Fit --------------------------------
k <- 10  # Number of components to consider
set.seed(123)
fit <- spca(x_train_pca, k = k, scale = TRUE, verbose = FALSE)
summary(fit)
### Step 2: Choose Optimal Alpha -----------------------------------
var.sp <- NULL
reg.par <- seq(0, 0.25, length = 20)  # Regularization parameters for alpha tuning
for (j in 1:20) {
fit <- spca(X, k = 5, alpha = reg.par[j], scale = TRUE, verbose = FALSE)
var.sp[j] <- sum(fit$sdev^2)  # Store explained variance
print(c(reg.par[j], sum(fit$sdev^2)))  # Print current alpha and explained variance
}
# Plot Explained Variance vs Regularization Parameter (Alpha)
par(mfrow = c(1, 1))
plot(reg.par, var.sp, type = "l", xlab = expression(lambda[1]), ylab = "Explained Variance", lwd = 2)
# Calculate variance reduction
explained_variance_drop <- 1 - var.sp / var.sp[1]
plot(diff(explained_variance_drop), type = "b", main = "Change in Explained Variance", ylab = "Difference", xlab = "Index of Regularization Parameter")
# Select alpha value based on plot (e.g., 9th parameter just before the elbow)
optimal_alpha_index <- 3
optimal_alpha <- reg.par[optimal_alpha_index]
cat("Optimal Alpha:", optimal_alpha, "\n")
### Step 3: Fit Sparse PCA with Optimal Alpha -----------------------
fit <- spca(x_train_spca, k = 5, alpha = optimal_alpha, scale = TRUE, verbose = FALSE)
### Step 4: Print Non-Zero Loadings ---------------------------------
V <- fit$loadings
for (j in 1:ncol(V)) {
ind <- which(V[, j] != 0)  # Get indices of non-zero loadings
v <- V[ind, j]  # Extract non-zero loadings
names(v) <- colnames(X)[ind]  # Name the loadings by the original feature names
cat("Component", j, ":\n")
print(v)
}
### Step 5: Regression on Principal Components ----------------------
# Standardize the input data and calculate principal component scores
pcData <- as.data.frame(scale(X) %*% V)
# Assuming you have a response variable `y_train`
# Fit a linear model using the principal components as predictors
y <- train_data$default_90  # Replace `default_90` with your target variable
m1 <- lm(y ~ ., data = pcData)
# Summary of regression model
summary(m1)
logistic_model_pca <- glm(y ~ ., data = pcData, family = binomial)
summary(logistic_model_pca)
### Evaluate Logistic Regression on Test Data ------------------
evaluate_model_pca <- function(model, test_data, target_col) {
predicted_prob <- predict(model, newdata = test_data, type = "response")
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
accuracy <- mean(predicted_class == test_data[[target_col]])
confusion <- confusionMatrix(factor(predicted_class), factor(test_data[[target_col]]))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score))
}
pca_results <- evaluate_model_pca(logistic_model_pca, pc_test, "default_90")
logistic_model_pca <- glm(y_train_pca ~ ., data = pcData, family = binomial)
summary(logistic_model_pca)
### Post Estimation---------------
# Predicted Probabilities for Test Data
predicted_prob_pca <- predict(logistic_model_pca, newdata = pcData, type = "response")
# True labels from train data (y_train_pca)
y_train_pca <- train_data$default_90  # Replace with the correct target column
# Evaluate Logistic Model Function
evaluate_logistic_model <- function(predicted_prob, true_labels, threshold = 0.5) {
predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
# Calculate accuracy
accuracy <- mean(predicted_class == true_labels)
# Calculate confusion matrix
confusion <- confusionMatrix(factor(predicted_class), factor(true_labels), positive = "1")
# Calculate F1 score
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
if (!is.na(precision) && !is.na(recall) && (precision + recall) != 0) {
f1_score <- 2 * (precision * recall) / (precision + recall)
} else {
f1_score <- NA
}
return(list(accuracy = accuracy, f1_score = f1_score, confusion = confusion))
}
# Evaluate Logistic Model
logistic_results <- evaluate_logistic_model(predicted_prob_pca, y_train_pca)
print(logistic_results$accuracy)
print(logistic_results$f1_score)
print(logistic_results$confusion)
# Evaluate Logistic Model
logistic_results <- evaluate_logistic_model(predicted_prob_pca, y_train_pca)
print(logistic_results$accuracy)
print(logistic_results$f1_score)
print(logistic_results$confusion)
### ROC and AUC
# ROC Curve and AUC for Logistic Model
roc_curve_pca <- roc(y_train_pca, predicted_prob_pca)
auc_value_pca <- auc(roc_curve_pca)
# Plot ROC Curve
plot(roc_curve_pca, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_pca, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# Filter predicted probabilities based on the true labels
scores_class0_pca <- predicted_prob_pca[y_train_pca == 1]
scores_class1_pca <- predicted_prob_pca[y_train_pca == 0]
# Generate PR Curve
pr_curve_pca <- pr.curve(scores.class0 = scores_class0_pca, scores.class1 = scores_class1_pca, curve = TRUE)
plot(pr_curve_pca, main = paste("Precision-Recall Curve (AUC =", round(pr_curve_pca$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
### Fine tunning threshold
# Threshold Optimization Function
optimize_threshold <- function(predicted_prob, true_labels, thresholds) {
results <- data.frame(Threshold = numeric(), Precision = numeric(), Recall = numeric(), F1_Score = numeric())
for (threshold in thresholds) {
# Convert probabilities to predicted classes
predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
# Calculate confusion matrix
confusion <- confusionMatrix(factor(predicted_class), factor(true_labels), positive = "1")
# Extract precision and recall
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
# Handle NA values
if (is.na(precision) || is.na(recall) || (precision + recall) == 0) {
f1_score <- NA
} else {
# Calculate F1 score
f1_score <- 2 * (precision * recall) / (precision + recall)
}
# Append results
results <- rbind(results, data.frame(Threshold = threshold, Precision = precision, Recall = recall, F1_Score = f1_score))
}
return(results)
}
# Define a range of thresholds to evaluate
thresholds <- seq(0.1, 0.9, by = 0.05)
# Optimize threshold
threshold_results <- optimize_threshold(predicted_prob_pca, y_train_pca, thresholds)
# Find the threshold that maximizes F1 score
optimal_threshold <- threshold_results$Threshold[which.max(threshold_results$F1_Score)]
cat("Optimal Threshold:", optimal_threshold, "\n")
# Plot Precision, Recall, and F1 Score vs Threshold
library(reshape2)
threshold_results_melted <- melt(threshold_results, id.vars = "Threshold", variable.name = "Metric", value.name = "Value")
ggplot(threshold_results_melted, aes(x = Threshold, y = Value, color = Metric)) +
geom_line(size = 1) +
theme_minimal() +
labs(title = "Precision, Recall, and F1 Score vs Threshold", x = "Threshold", y = "Value") +
scale_color_manual(values = c("blue", "red", "green"))
# Predicted classes using the optimal threshold
predicted_class_optimal <- ifelse(predicted_prob_pca > optimal_threshold, 1, 0)
# Find the threshold that maximizes F1 score
optimal_threshold <- threshold_results$Threshold[which.max(threshold_results$F1_Score)]
cat("Optimal Threshold:", optimal_threshold, "\n")
threshold_results_melted <- melt(threshold_results, id.vars = "Threshold", variable.name = "Metric", value.name = "Value")
ggplot(threshold_results_melted, aes(x = Threshold, y = Value, color = Metric)) +
geom_line(size = 1) +
theme_minimal() +
labs(title = "Precision, Recall, and F1 Score vs Threshold", x = "Threshold", y = "Value") +
scale_color_manual(values = c("blue", "red", "green"))
# Predicted classes using the optimal threshold
predicted_class_optimal <- ifelse(predicted_prob_pca > optimal_threshold, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class_optimal, Actual = y_train_pca)
# Convert to a data frame for plotting
conf_matrix_df <- as.data.frame(conf_matrix)
# Plot Confusion Matrix Heatmap
ggplot(conf_matrix_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
install.packages("e1071")
library(e1071) # for SVM
## SVM---------------
# Train SVM Model with Linear Kernel
svm_model <- svm(default_90 ~ ., data = train_data, kernel = "linear", probability = TRUE)
# Summary of SVM Model
summary(svm_model)
# Define Evaluation Function for SVM Model
evaluate_svm_model <- function(model, test_data, target_col) {
# Predict class probabilities for the test data
predicted_prob <- attr(predict(model, newdata = test_data, probability = TRUE), "probabilities")[, 2]
# Predict class labels using 0.5 threshold
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Calculate accuracy
accuracy <- mean(predicted_class == test_data[[target_col]])
# Calculate confusion matrix
confusion <- confusionMatrix(factor(predicted_class), factor(test_data[[target_col]]))
# Calculate F1 score
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
if (!is.na(precision) && !is.na(recall) && (precision + recall) != 0) {
f1_score <- 2 * (precision * recall) / (precision + recall)
} else {
f1_score <- NA
}
return(list(accuracy = accuracy, f1_score = f1_score, confusion = confusion))
}
### Evaluate SVM Model----------
svm_results <- evaluate_svm_model(svm_model, test_data, "default_90")
print(svm_results$f1_score)
print(svm_results$confusion)
# Define Evaluation Function for SVM Model
evaluate_svm_model <- function(model, test_data, target_col) {
# Predict class probabilities for the test data
predicted_prob <- attr(predict(model, newdata = test_data, probability = TRUE), "probabilities")[, 2]
# Predict class labels using 0.5 threshold
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Calculate accuracy
accuracy <- mean(predicted_class == test_data[[target_col]])
# Calculate confusion matrix
confusion <- confusionMatrix(factor(predicted_class), factor(test_data[[target_col]]))
# Calculate F1 score
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
if (!is.na(precision) && !is.na(recall) && (precision + recall) != 0) {
f1_score <- 2 * (precision * recall) / (precision + recall)
} else {
f1_score <- NA
}
return(list(accuracy = accuracy, f1_score = f1_score, confusion = confusion))
}
### Evaluate SVM Model----------
svm_results <- evaluate_svm_model(svm_model, test_data, "default_90")
classifier <- svm(formula = default_90 ~ .,
data = train_data,
type = 'C-classification',
kernel = 'linear')
## SVM---------------
# needs scaling
# Feature Scaling
train_data
## SVM---------------
# needs scaling
# Feature Scaling
train_data[ncol(train_data)]
train_data[-3] = scale(train_data[-ncol(train_data)])
test_data[-3] = scale(test_data[-ncol(train_data)])
## SVM---------------
# needs scaling
# Feature Scaling
# Extract feature names excluding the target variable
features <- colnames(train_data)[colnames(train_data) != "default_90"]
# Step 1: Scale the Training Set
# Calculate mean and standard deviation for scaling
scaling_params <- preProcess(train_data[, features], method = c("center", "scale"))
X_train_scaled
## SVM---------------
# needs scaling (done for group lasso)
# Combine X_train_scaled and y_train to create the training dataset in data frame format
train_data_svm <- as.data.frame(X_train_scaled)
train_data_svm$default_90 <- factor(y_train)
# Train SVM Model with Linear Kernel using Scaled Data
svm_model <- svm(default_90 ~ ., data = train_data_svm, kernel = "linear", probability = TRUE)
# Evaluate the SVM model on scaled test set
# Combine X_test_scaled and y_test to create the test dataset in data frame format
test_data_svm <- as.data.frame(X_test_scaled)
test_data_svm$default_90 <- factor(y_test)
# Predict class probabilities for the test set
predicted_prob_svm <- attr(predict(svm_model, newdata = test_data_svm, probability = TRUE), "probabilities")[, 2]
# Predicted class labels using threshold of 0.5
predicted_class_svm <- ifelse(predicted_prob_svm > 0.5, 1, -1)
# Calculate accuracy, confusion matrix, and F1 score
evaluate_svm_model <- function(model, predicted_class, true_labels) {
# Calculate accuracy
accuracy <- mean(predicted_class == true_labels)
# Calculate confusion matrix
confusion <- confusionMatrix(factor(predicted_class), factor(true_labels))
# Calculate F1 score
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
if (!is.na(precision) && !is.na(recall) && (precision + recall) != 0) {
f1_score <- 2 * (precision * recall) / (precision + recall)
} else {
f1_score <- NA
}
return(list(accuracy = accuracy, f1_score = f1_score, confusion = confusion))
}
# Evaluate SVM Model
svm_results <- evaluate_svm_model(svm_model, predicted_class_svm, y_test)
print(svm_results$accuracy)
print(svm_results$f1_score)
print(svm_results$confusion)
# Calculate accuracy, confusion matrix, and F1 score
evaluate_svm_model <- function(model, predicted_class, true_labels) {
# Calculate accuracy
accuracy <- mean(predicted_class == true_labels)
# Calculate confusion matrix
confusion <- confusionMatrix(factor(predicted_class), factor(true_labels))
# Calculate F1 score
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
if (!is.na(precision) && !is.na(recall) && (precision + recall) != 0) {
f1_score <- 2 * (precision * recall) / (precision + recall)
} else {
f1_score <- NA
}
return(list(accuracy = accuracy, f1_score = f1_score, confusion = confusion))
}
# Evaluate SVM Model
svm_results <- evaluate_svm_model(svm_model, predicted_class_svm, y_test)
# Train SVM Model with Linear Kernel using Scaled Data
svm_model <- svm(default_90 ~ ., data = train_data_svm, kernel = "linear", probability = TRUE)
# Evaluate the SVM model on scaled test set
# Combine X_test_scaled and y_test to create the test dataset in data frame format
test_data_svm <- as.data.frame(X_test_scaled)
test_data_svm$default_90 <- factor(y_test)
# Predict class probabilities for the test set
predicted_prob_svm <- attr(predict(svm_model, newdata = test_data_svm, probability = TRUE), "probabilities")[, 2]
predicted_probs_svm
# Evaluate the SVM model on scaled test set
# Combine X_test_scaled and y_test to create the test dataset in data frame format
test_data_svm <- as.data.frame(X_test_scaled)
test_data_svm$default_90 <- factor(y_test)
# Predict class probabilities for the test set
predicted_prob_svm <- attr(predict(svm_model, newdata = test_data_svm, probability = TRUE), "probabilities")[, 2]
predicted_probs_svm
# Predict class probabilities for the test set
predicted_prob_svm <- attr(predict(svm_model, newdata = test_data_svm, probability = TRUE), "probabilities")[, 2]
predicted_probs_svm
print(predicted_prob_svm)
# Predicted class labels using threshold of 0.5
predicted_class_svm <- ifelse(predicted_prob_svm > 0.5, 1, -1)
mean(predicted_class_svm)
predicted_class_svm
# True labels
true_labels <- test_data$default_90
true_labels
# Convert predicted classes to a factor (for compatibility with confusionMatrix)
predicted_class_svm <- factor(predicted_class_svm, levels = c(-1, 1))
y_test_factor <- factor(y_test, levels = c(-1, 1))
# Evaluate model
# Calculate confusion matrix
confusion <- confusionMatrix(predicted_class_svm, y_test_factor, positive = "1")
# Convert predicted classes to a factor (for compatibility with confusionMatrix)
predicted_class_svm <- factor(predicted_class_svm, levels = c(-1, 1))
y_test_factor <- factor(y_test, levels = c(-1, 1))
# Evaluate model
# Calculate confusion matrix
confusion <- confusionMatrix(predicted_class_svm, y_test_factor, positive = "1")
# Evaluate model
# Calculate confusion matrix
library(caret)
confusion <- confusionMatrix(predicted_class_svm, y_test_factor, positive = "1")
# Calculate accuracy
accuracy <- sum(predicted_class_svm == y_test_factor) / length(y_test)
cat("Accuracy:", accuracy, "\n")
# Extract precision and recall from the confusion matrix
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
# Calculate F1 score
if (!is.na(precision) && !is.na(recall) && (precision + recall) != 0) {
f1_score <- 2 * (precision * recall) / (precision + recall)
} else {
f1_score <- NA
}
cat("F1 Score:", f1_score, "\n")
cat("F1 Score:", f1_score, "\n")
