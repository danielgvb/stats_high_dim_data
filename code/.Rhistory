# data <- data %>%
#   mutate(max_education = case_when(
#     max_education == "primaria" ~ 1,
#     max_education == "secundaria" ~ 2,
#     max_education == "técnico" ~ 3,
#     max_education == "tecnólogo" ~ 4,
#     max_education == "Universitario" ~ 5,
#     max_education == "Posgrado" ~ 6,
#     TRUE ~ NA_real_
#   ))
## Create Derived Variables
data <- data %>%
mutate(
#installment_periodic = installment / periodicity_num,
time_difference_days = as.numeric(difftime(as.Date(date_limit), as.Date(date_approval), units = "days"))
)
## Date furhter information
# First create year, month, day, weekday
# Extract features from the date-time variables
# date approval
data$m_date_approval <- format(data$date_approval, "%m")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$m_date_limit <- format(data$date_limit, "%m")
data$wd_date_limit <- weekdays(data$date_limit)
## Convert POSIXct to numeric
## Identify columns of type POSIXct
posix_columns <- sapply(data, function(col) inherits(col, "POSIXct"))
## Convert POSIXct columns to numeric
data[posix_columns] <- lapply(data[posix_columns], as.numeric)
## Convert Characters to Factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
## Convert all numeric to log
# Identify numeric columns in the dataframe
numeric_columns <- sapply(data, is.numeric)
# Exclude the specified columns
exclude_columns <- c("default_90", "dtf_apporval_date")
columns_to_transform <- setdiff(names(data)[numeric_columns], exclude_columns)
# Apply the natural logarithm to the selected columns, adding 1 to handle zeros
data[columns_to_transform] <- lapply(data[columns_to_transform], function(col) log(col + 1))
# Move "default_90" to the last column
data <- data[, c(setdiff(names(data), "default_90"), "default_90")]
#REMOVE AGE because of Noise--------------
#data <- data[, !names(data) %in% c("age")]
# Train-Test Split --------------------------------------
set.seed(123)
split <- sample.split(data$default_90, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)
## Numeric Variables------------
numeric_data <- train_data %>% select(where(is.numeric))
## Numeric Variables------------
numeric_data <- train_data[, sapply(train_data, is.numeric)]
#numeric_data <- train_data %>% select(where(is.numeric))
skim(numeric_data)
### Histograms
for (col_name in colnames(numeric_data)) {
hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
xlab = col_name, col = "lightblue", border = "black")
}
### Density Plots--------------
for (col_name in colnames(numeric_data)) {
print(ggplot(train_data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +
labs(title = paste("Density Plot of", col_name), x = col_name, fill = "Target") +
theme_minimal())
}
### Non-Numeric Variables------------
non_numeric_data <- train_data %>% select(where(~ !is.numeric(.)))
unique_counts <- sapply(non_numeric_data, function(x) length(unique(x)))
mode_values <- sapply(non_numeric_data, function(x) names(which.max(table(x))))
print(mode_values)
# Load Required Libraries --------------------------------
install.packages("dplyr")
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC)
library(car)
library(gglasso)
library(doParallel)
library(mgcv) # for GAM
library(sparsepca) # for sparse pca
# Set Working Directory ---------------------------------
data_path <- "C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx"
# Import Data -------------------------------------------
data <- read_excel(data_path)
# Data Preprocessing ------------------------------------
## Remove ID Variables
id_vars <- c("Código de Crédito", "ID Cliente", "No Pagaré Rotativo")
data <- data[, !names(data) %in% id_vars]
## Remove Single-Value Columns
single_value_vars <- c("Clasificación Tipo Crédito")
data <- data[, !names(data) %in% single_value_vars]
## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity", "credit_duration", "date_limit",
"dtf_approval_date", "fx_approval_date", "default_90")
if (length(friendly_names) == ncol(data)) {
colnames(data) <- friendly_names
} else {
stop("Column name mismatch.")
}
## Handle Missing Values
na_counts <- colSums(is.na(data))
print(na_counts)
# Transformations ---------------------------------------
## Filter Credit Limit > 50,000
#data <- data[data$credit_limit > 50000, ]
## Map Periodicity to Numeric
# data <- data %>%
#   mutate(periodicity_num = case_when(
#     periodicity == "Mensual" ~ 30,
#     periodicity == "Bimensual" ~ 60,
#     periodicity == "Quincenal" ~ 15,
#     TRUE ~ NA_real_
#   )) %>%
#   select(-periodicity)
## Map Education Levels to Numeric
# If we remove this then the model does not find min lambda
# data <- data %>%
#   mutate(max_education = case_when(
#     max_education == "primaria" ~ 1,
#     max_education == "secundaria" ~ 2,
#     max_education == "técnico" ~ 3,
#     max_education == "tecnólogo" ~ 4,
#     max_education == "Universitario" ~ 5,
#     max_education == "Posgrado" ~ 6,
#     TRUE ~ NA_real_
#   ))
## Create Derived Variables
data <- data %>%
mutate(
#installment_periodic = installment / periodicity_num,
time_difference_days = as.numeric(difftime(as.Date(date_limit), as.Date(date_approval), units = "days"))
)
## Date furhter information
# First create year, month, day, weekday
# Extract features from the date-time variables
# date approval
data$m_date_approval <- format(data$date_approval, "%m")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$m_date_limit <- format(data$date_limit, "%m")
data$wd_date_limit <- weekdays(data$date_limit)
## Convert POSIXct to numeric
## Identify columns of type POSIXct
posix_columns <- sapply(data, function(col) inherits(col, "POSIXct"))
## Convert POSIXct columns to numeric
data[posix_columns] <- lapply(data[posix_columns], as.numeric)
## Convert Characters to Factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
## Convert all numeric to log
# Identify numeric columns in the dataframe
numeric_columns <- sapply(data, is.numeric)
# Exclude the specified columns
exclude_columns <- c("default_90", "dtf_apporval_date")
columns_to_transform <- setdiff(names(data)[numeric_columns], exclude_columns)
# Apply the natural logarithm to the selected columns, adding 1 to handle zeros
data[columns_to_transform] <- lapply(data[columns_to_transform], function(col) log(col + 1))
# Move "default_90" to the last column
data <- data[, c(setdiff(names(data), "default_90"), "default_90")]
#REMOVE AGE because of Noise--------------
#data <- data[, !names(data) %in% c("age")]
# Train-Test Split --------------------------------------
set.seed(123)
split <- sample.split(data$default_90, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)
# Exploratory Data Analysis -----------------------------
## Numeric Variables------------
numeric_data <- train_data[, sapply(train_data, is.numeric)]
#numeric_data <- train_data %>% select(where(is.numeric))
skim(numeric_data)
### Histograms
for (col_name in colnames(numeric_data)) {
hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
xlab = col_name, col = "lightblue", border = "black")
}
### Density Plots--------------
for (col_name in colnames(numeric_data)) {
print(ggplot(train_data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +
labs(title = paste("Density Plot of", col_name), x = col_name, fill = "Target") +
theme_minimal())
}
### Non-Numeric Variables------------
non_numeric_data <- train_data %>% select(where(~ !is.numeric(.)))
unique_counts <- sapply(non_numeric_data, function(x) length(unique(x)))
mode_values <- sapply(non_numeric_data, function(x) names(which.max(table(x))))
print(mode_values)
cor_matrix <- cor(numeric_data, use = "complete.obs")
cor_matrix
melted_cor_matrix <- melt(cor_matrix)
ggplot(data = melted_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
theme_minimal() +
coord_fixed() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
for (col_name in colnames(non_numeric_data)) {
print(
ggplot(train_data, aes_string(x = col_name, fill = "factor(default_90)")) +
geom_bar(position = "fill") +  # Use position = "dodge" for side-by-side bars
labs(title = paste("Bar Plot of", col_name, "by Target"),
x = col_name, y = "Proportion", fill = "Target") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
p_values <- c()
# Perform Chi-square tests and collect p-values
for (col_name in colnames(non_numeric_data)) {
contingency_table <- table(non_numeric_data[[col_name]], train_data$default_90)
chi2_result <- chisq.test(contingency_table)
# Store the p-value
p_values <- c(p_values, chi2_result$p.value)
}
contingency_table
chi2_results_df <- data.frame(
Variable = colnames(non_numeric_data),
P_value = p_values
)
# Plot the p-values
ggplot(chi2_results_df, aes(x = reorder(Variable, -P_value), y = P_value)) +
geom_bar(stat = "identity", fill = "skyblue") +
geom_hline(yintercept = 0.05, color = "red", linetype = "dashed") +  # Significance threshold
labs(title = "P-values from Chi-square Tests for Non-numeric Variables vs default_90",
x = "Variable",
y = "P-value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
logistic_model <- glm(default_90 ~ ., data = train_data, family = binomial)
summary(logistic_model)
### Evaluate Logistic Regression------------
evaluate_model <- function(model, test_data, target_col) {
predicted_prob <- predict(model, newdata = test_data, type = "response")
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
accuracy <- mean(predicted_class == test_data[[target_col]])
confusion <- confusionMatrix(factor(predicted_class), factor(test_data[[target_col]]))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score))
}
logistic_results <- evaluate_model(logistic_model, test_data, "default_90")
print(logistic_results)
# f1 score > 0.7 is good
### Post-Estimation Plots---------------
par(mfrow = c(2,2))
plot(logistic_model)
#### 1. ROC Curve and AUC ------------------------------------
par(mfrow = c(1,1))
# Predicted probabilities
predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
# ROC and AUC
roc_curve <- roc(test_data$default_90, predicted_prob)
auc_value <- auc(roc_curve)
# Plot ROC Curve
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
View(train_data)
# Load Required Libraries --------------------------------
install.packages("dplyr")
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC)
library(car)
library(gglasso)
library(doParallel)
library(mgcv) # for GAM
library(sparsepca) # for sparse pca
# Set Working Directory ---------------------------------
data_path <- "C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx"
# Import Data -------------------------------------------
data <- read_excel(data_path)
# Data Preprocessing ------------------------------------
## Remove ID Variables
id_vars <- c("Código de Crédito", "ID Cliente", "No Pagaré Rotativo")
data <- data[, !names(data) %in% id_vars]
## Remove Single-Value Columns
single_value_vars <- c("Clasificación Tipo Crédito")
data <- data[, !names(data) %in% single_value_vars]
## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity", "credit_duration", "date_limit",
"dtf_approval_date", "fx_approval_date", "default_90")
if (length(friendly_names) == ncol(data)) {
colnames(data) <- friendly_names
} else {
stop("Column name mismatch.")
}
## Handle Missing Values
na_counts <- colSums(is.na(data))
print(na_counts)
# Transformations ---------------------------------------
## Filter Credit Limit > 50,000
#data <- data[data$credit_limit > 50000, ]
## Map Periodicity to Numeric
# data <- data %>%
#   mutate(periodicity_num = case_when(
#     periodicity == "Mensual" ~ 30,
#     periodicity == "Bimensual" ~ 60,
#     periodicity == "Quincenal" ~ 15,
#     TRUE ~ NA_real_
#   )) %>%
#   select(-periodicity)
## Map Education Levels to Numeric
# If we remove this then the model does not find min lambda
# data <- data %>%
#   mutate(max_education = case_when(
#     max_education == "primaria" ~ 1,
#     max_education == "secundaria" ~ 2,
#     max_education == "técnico" ~ 3,
#     max_education == "tecnólogo" ~ 4,
#     max_education == "Universitario" ~ 5,
#     max_education == "Posgrado" ~ 6,
#     TRUE ~ NA_real_
#   ))
## Create Derived Variables
data <- data %>%
mutate(
#installment_periodic = installment / periodicity_num,
time_difference_days = as.numeric(difftime(as.Date(date_limit), as.Date(date_approval), units = "days"))
)
## Date furhter information
# First create year, month, day, weekday
# Extract features from the date-time variables
# date approval
data$m_date_approval <- format(data$date_approval, "%m")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$m_date_limit <- format(data$date_limit, "%m")
data$wd_date_limit <- weekdays(data$date_limit)
## Convert POSIXct to numeric
## Identify columns of type POSIXct
posix_columns <- sapply(data, function(col) inherits(col, "POSIXct"))
## Convert POSIXct columns to numeric
data[posix_columns] <- lapply(data[posix_columns], as.numeric)
## Convert Characters to Factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
## Convert all numeric to log
# Identify numeric columns in the dataframe
numeric_columns <- sapply(data, is.numeric)
# Exclude the specified columns
exclude_columns <- c("default_90", "dtf_apporval_date")
columns_to_transform <- setdiff(names(data)[numeric_columns], exclude_columns)
# Apply the natural logarithm to the selected columns, adding 1 to handle zeros
data[columns_to_transform] <- lapply(data[columns_to_transform], function(col) log(col + 1))
# Move "default_90" to the last column
data <- data[, c(setdiff(names(data), "default_90"), "default_90")]
#REMOVE AGE because of Noise--------------
#data <- data[, !names(data) %in% c("age")]
# Train-Test Split --------------------------------------
set.seed(123)
split <- sample.split(data$default_90, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)
# Exploratory Data Analysis -----------------------------
## Numeric Variables------------
numeric_data <- train_data[, sapply(train_data, is.numeric)]
#numeric_data <- train_data %>% select(where(is.numeric))
skim(numeric_data)
### Histograms
for (col_name in colnames(numeric_data)) {
hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
xlab = col_name, col = "lightblue", border = "black")
}
### Density Plots--------------
for (col_name in colnames(numeric_data)) {
print(ggplot(train_data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +
labs(title = paste("Density Plot of", col_name), x = col_name, fill = "Target") +
theme_minimal())
}
# Load Required Libraries --------------------------------
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC)
library(car)
library(gglasso)
library(doParallel)
library(mgcv) # for GAM
library(sparsepca) # for sparse pca
# Set Working Directory ---------------------------------
data_path <- "C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data_syntetic_age.xlsx"
# Import Data -------------------------------------------
data <- read_excel(data_path)
# Data Preprocessing ------------------------------------
## Remove ID Variables
id_vars <- c("Código de Crédito", "ID Cliente", "No Pagaré Rotativo")
data <- data[, !names(data) %in% id_vars]
## Remove Single-Value Columns
single_value_vars <- c("Clasificación Tipo Crédito")
data <- data[, !names(data) %in% single_value_vars]
## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity", "credit_duration", "date_limit",
"dtf_approval_date", "fx_approval_date", "default_90")
if (length(friendly_names) == ncol(data)) {
colnames(data) <- friendly_names
} else {
stop("Column name mismatch.")
}
## Handle Missing Values
na_counts <- colSums(is.na(data))
print(na_counts)
# Transformations ---------------------------------------
## Filter Credit Limit > 50,000
#data <- data[data$credit_limit > 50000, ]
## Map Periodicity to Numeric
# data <- data %>%
#   mutate(periodicity_num = case_when(
#     periodicity == "Mensual" ~ 30,
#     periodicity == "Bimensual" ~ 60,
#     periodicity == "Quincenal" ~ 15,
#     TRUE ~ NA_real_
#   )) %>%
#   select(-periodicity)
## Map Education Levels to Numeric
# If we remove this then the model does not find min lambda
# data <- data %>%
#   mutate(max_education = case_when(
#     max_education == "primaria" ~ 1,
#     max_education == "secundaria" ~ 2,
#     max_education == "técnico" ~ 3,
#     max_education == "tecnólogo" ~ 4,
#     max_education == "Universitario" ~ 5,
#     max_education == "Posgrado" ~ 6,
#     TRUE ~ NA_real_
#   ))
## Create Derived Variables
data <- data %>%
mutate(
#installment_periodic = installment / periodicity_num,
time_difference_days = as.numeric(difftime(as.Date(date_limit), as.Date(date_approval), units = "days"))
)
## Date furhter information
# First create year, month, day, weekday
# Extract features from the date-time variables
# date approval
data$m_date_approval <- format(data$date_approval, "%m")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$m_date_limit <- format(data$date_limit, "%m")
data$wd_date_limit <- weekdays(data$date_limit)
## Convert POSIXct to numeric
## Identify columns of type POSIXct
posix_columns <- sapply(data, function(col) inherits(col, "POSIXct"))
## Convert POSIXct columns to numeric
data[posix_columns] <- lapply(data[posix_columns], as.numeric)
## Convert Characters to Factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
## Convert all numeric to log
# Identify numeric columns in the dataframe
numeric_columns <- sapply(data, is.numeric)
# Exclude the specified columns
exclude_columns <- c("default_90", "dtf_apporval_date")
columns_to_transform <- setdiff(names(data)[numeric_columns], exclude_columns)
# Apply the natural logarithm to the selected columns, adding 1 to handle zeros
data[columns_to_transform] <- lapply(data[columns_to_transform], function(col) log(col + 1))
# Move "default_90" to the last column
data <- data[, c(setdiff(names(data), "default_90"), "default_90")]
#REMOVE AGE because of Noise--------------
data <- data[, !names(data) %in% c("age")]
# Train-Test Split --------------------------------------
set.seed(123)
split <- sample.split(data$default_90, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)
# Exploratory Data Analysis -----------------------------
## Numeric Variables------------
numeric_data <- train_data %>% select(where(is.numeric))
skim(numeric_data)
