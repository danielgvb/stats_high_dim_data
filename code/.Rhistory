columns_to_transform <- setdiff(names(data)[numeric_columns], exclude_columns)
## Log transform------------
# Apply the natural logarithm to the selected columns, adding 1 to handle zeros
data[columns_to_transform] <- lapply(data[columns_to_transform], function(col) log(col + 1))
# Move "default_90" to the last column
data <- data[, c(setdiff(names(data), "default_90"), "default_90")]
#REMOVE AGE because of Noise--------------
data <- data[, !names(data) %in% c("age")]
# Train-Test Split --------------------------------------
set.seed(123)
split <- sample.split(data$default_90, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)
# Exploratory Data Analysis -----------------------------
## Numeric Variables------------
numeric_data <- train_data %>% select(where(is.numeric))
skim(numeric_data)
### Histograms
for (col_name in colnames(numeric_data)) {
hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
xlab = col_name, col = "lightblue", border = "black")
}
### Box Plots---------------
for (col_name in colnames(numeric_data)) {
print(ggplot(train_data, aes(x = factor(default_90), y = .data[[col_name]], fill = factor(default_90))) +
geom_boxplot(alpha = 0.7) +
labs(title = paste("Boxplot of", col_name, "by Target"),
x = "Target",
y = col_name,
fill = "Target") +
theme_minimal())
}
### Density Plots--------------
for (col_name in colnames(numeric_data)) {
print(ggplot(train_data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +
labs(title = paste("Density Plot of", col_name), x = col_name, fill = "Target") +
theme_minimal())
}
### Non-Numeric Variables------------
non_numeric_data <- train_data %>% select(where(~ !is.numeric(.)))
unique_counts <- sapply(non_numeric_data, function(x) length(unique(x)))
mode_values <- sapply(non_numeric_data, function(x) names(which.max(table(x))))
print(mode_values)
### Correlation Analysis ----------------------------------
cor_matrix <- cor(numeric_data, use = "complete.obs")
cor_matrix
melted_cor_matrix <- melt(cor_matrix)
ggplot(data = melted_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
theme_minimal() +
coord_fixed() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
### Barplots by Target-------
for (col_name in colnames(non_numeric_data)) {
print(
ggplot(train_data, aes_string(x = col_name, fill = "factor(default_90)")) +
geom_bar(position = "fill") +  # Use position = "dodge" for side-by-side bars
labs(title = paste("Bar Plot of", col_name, "by Target"),
x = col_name, y = "Proportion", fill = "Target") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
# All bimensual are defaulty (is just 1)
### Chi Squared------------
# The output will show the Chi-square statistic, degrees of freedom, and the p-value.
# A p-value less than 0.05 indicates that there is a significant association
# between the non-numeric variable and the target variable default_90.
# Create a vector to store p-values
p_values <- c()
# Perform Chi-square tests and collect p-values
for (col_name in colnames(non_numeric_data)) {
contingency_table <- table(non_numeric_data[[col_name]], train_data$default_90)
chi2_result <- chisq.test(contingency_table)
# Store the p-value
p_values <- c(p_values, chi2_result$p.value)
}
contingency_table
# Create a data frame for plotting
chi2_results_df <- data.frame(
Variable = colnames(non_numeric_data),
P_value = p_values
)
# Plot the p-values
ggplot(chi2_results_df, aes(x = reorder(Variable, -P_value), y = P_value)) +
geom_bar(stat = "identity", fill = "skyblue") +
geom_hline(yintercept = 0.05, color = "red", linetype = "dashed") +  # Significance threshold
labs(title = "P-values from Chi-square Tests for Non-numeric Variables vs default_90",
x = "Variable",
y = "P-value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Model Training and Evaluation ------------------------
## Logistic Regression----------------
logistic_model <- glm(default_90 ~ ., data = train_data, family = binomial)
summary(logistic_model)
### Evaluate Logistic Regression------------
evaluate_model <- function(model, test_data, target_col) {
predicted_prob <- predict(model, newdata = test_data, type = "response")
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
accuracy <- mean(predicted_class == test_data[[target_col]])
confusion <- confusionMatrix(factor(predicted_class), factor(test_data[[target_col]]))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score))
}
logistic_results <- evaluate_model(logistic_model, test_data, "default_90")
print(logistic_results)
# f1 score > 0.7 is good
### Post-Estimation Plots---------------
par(mfrow = c(2,2))
plot(logistic_model)
#### 1. ROC Curve and AUC ------------------------------------
par(mfrow = c(1,1))
# Predicted probabilities
predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
# ROC and AUC
roc_curve <- roc(test_data$default_90, predicted_prob)
auc_value <- auc(roc_curve)
# Plot ROC Curve
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# auc > 0.7 is reasonable model
#### 2. Confusion Matrix Heatmap -----------------------------
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
#### 3. Coefficient Plot -------------------------------------
# Extract coefficients
coefficients <- summary(logistic_model)$coefficients
coef_data <- as.data.frame(coefficients)
coef_data$Variable <- rownames(coefficients)
rownames(coef_data) <- NULL
# Plot coefficients
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Coefficient Plot", x = "Variable", y = "Estimate") +
theme_minimal()
#### 4. Calibration Curve ------------------------------------
# Bin predicted probabilities
calibration_data <- data.frame(
Predicted = predicted_prob,
Observed = test_data$default_90
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
# Mean predicted and observed probabilities by bin
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
# Plot calibration curve
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
## Stepwise Logistic Regression----------------
library(MASS)  # For stepwise regression functions
### 1. Forward Selection ------------------------
# Start with an empty model
start_model <- glm(default_90 ~ 1, data = train_data, family = binomial)
# Define the full model with all predictors
full_model <- glm(default_90 ~ ., data = train_data, family = binomial)
# Perform forward selection
forward_model <- step(
start_model,
scope = list(lower = start_model, upper = full_model),
direction = "forward",
trace = 0
)
# Evaluate Forward Selection Model
forward_results <- evaluate_model(forward_model, test_data, "default_90")
print(forward_results)
### 2. Backward Elimination ---------------------
# Start with the full model
backward_model <- step(
full_model,
direction = "backward",
trace = 0
)
# Evaluate Backward Elimination Model
backward_results <- evaluate_model(backward_model, test_data, "default_90")
print(backward_results)
### Post-Estimation Plots for Forward Model ----------------
# Predicted probabilities
predicted_prob <- predict(forward_model, newdata = test_data, type = "response")
#### 1. ROC Curve and AUC for Forward Model----------
roc_curve <- roc(test_data$default_90, predicted_prob)
auc_value <- auc(roc_curve)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### 2. Confusion Matrix Heatmap for Forward Model---------
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap (Forward Model)", x = "Actual", y = "Predicted") +
theme_minimal()
#### 3. Coefficient Plot for Forward Model--------------
coef_data <- as.data.frame(summary(forward_model)$coefficients)
coef_data$Variable <- rownames(coef_data)
rownames(coef_data) <- NULL
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Coefficient Plot (Forward Model)", x = "Variable", y = "Estimate") +
theme_minimal()
#### 4. Calibration Curve for Forward Model-------------
calibration_data <- data.frame(
Predicted = predicted_prob,
Observed = test_data$default_90
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Forward Model)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
### Post-Estimation Plots for Backward Model ----------------
# Predicted probabilities
predicted_prob <- predict(backward_model, newdata = test_data, type = "response")
#### 1. ROC Curve and AUC for Backward Model ----------
roc_curve <- roc(test_data$default_90, predicted_prob)
auc_value <- auc(roc_curve)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### 2. Confusion Matrix Heatmap for Backward Model ---------
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap (Backward Model)", x = "Actual", y = "Predicted") +
theme_minimal()
#### 3. Coefficient Plot for Backward Model --------------
coef_data <- as.data.frame(summary(backward_model)$coefficients)
coef_data$Variable <- rownames(coef_data)
rownames(coef_data) <- NULL
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Coefficient Plot (Backward Model)", x = "Variable", y = "Estimate") +
theme_minimal()
#### 4. Calibration Curve for Backward Model -------------
calibration_data <- data.frame(
Predicted = predicted_prob,
Observed = test_data$default_90
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Backward Model)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
## Lasso Logistic Regression--------------
### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
y_train <- train_data$default_90
x_test <- as.matrix(test_data[, colnames(test_data) != "default_90"])
y_test <- test_data$default_90
# scale predictors
#x_train <- scale(x_train)
#x_test <- scale(x_test)
### 2. Perform Cross-Validation for Lasso Logistic Regression --
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial",
standarize = T,alpha = 1, nfolds = 10)
# Plot of Cross-Validation Results
# Optimal lambda
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
# Plot deviance against log(lambda)
par(mfrow= c(1,1))
plot(cv_lasso, main = "Cross-Validation for Lasso Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
# We find the lambda to the top left, meaning it could be further left:
# Define a custom lambda sequence (smaller values)
# lambda_grid <- 10^seq(-10, 2, length = 100)  # Adjust range to include smaller lambda values
#
# # Perform Cross-Validation with custom lambda grid
# set.seed(123)
# cv_lasso <- cv.glmnet(
#   x_train, y_train,
#   family = "binomial",
#   alpha = 1,
#   lambda = lambda_grid,
#   nfolds = 10
# )
#
# # Optimal lambdas
# lambda_min <- cv_lasso$lambda.min
# lambda_1se <- cv_lasso$lambda.1se
#
# # Plot the updated deviance vs log(lambda) with the extended search range
# plot(cv_lasso, main = "Extended Cross-Validation for Lasso Logistic Regression")
# abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
# abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
# legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
#
#
# # Lambda is still max left, there is something off with the model
# Optimal lambda
lambda_optimal <- cv_lasso$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")
### 3. Fit the Final Model with Optimal Lambda -----------------
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = lambda_optimal)
###  4. Evaluate the Model with Adjusted Threshold ------------
evaluate_lasso <- function(predicted_prob, y_test, threshold = 0.3) {
# Predicted classes with custom threshold
predicted_class <- ifelse(predicted_prob > threshold, 1, 0)
# Calculate accuracy
accuracy <- mean(predicted_class == y_test)
# Create confusion matrix
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
# Calculate AUC
auc_value <- auc(roc(y_test, predicted_prob))
# Extract precision and recall, handling NA values
precision <- confusion$byClass["Pos Pred Value"]
recall <- confusion$byClass["Sensitivity"]
if (is.na(precision) || is.na(recall) || (precision + recall) == 0) {
f1_score <- NA
} else {
f1_score <- 2 * (precision * recall) / (precision + recall)
}
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value, confusion = confusion))
}
### 5. Predicted Probabilities and Evaluation -----------------
predicted_prob <- predict(lasso_model, s = lambda_optimal, newx = x_test, type = "response")
lasso_results <- evaluate_lasso(predicted_prob, y_test, threshold = 0.5)
print(lasso_results)
### 6. Post-Estimation Plots ----------------------------------
#### (1) ROC Curve and AUC---------
roc_curve <- roc(y_test, predicted_prob)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(lasso_results$auc, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### (2) Confusion Matrix Heatmap------------
predicted_class <- ifelse(predicted_prob > 0.3, 1, 0)
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
#### (3) Coefficient Plot ----------------------------------------
coef_data <- as.data.frame(as.matrix(coef(lasso_model, s = lambda_optimal)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Lasso Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
#### (4) Calibration Curve ---------------------------------------
calibration_data <- data.frame(
Predicted = as.vector(predicted_prob),
Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
# Calculate mean predicted and observed probabilities by bin
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
# Plot calibration curve
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
## Elastic Net Logistic Regression--------------
### 1. Prepare Data for glmnet --------------------------------
### 2. Perform Cross-Validation for Elastic Net Logistic Regression --
# Elastic net uses `alpha` to mix lasso (alpha = 1) and ridge (alpha = 0)
set.seed(123)
cv_elastic_net <- cv.glmnet(
x_train, y_train,
standarize = T,
family = "binomial",
alpha = 0.5,        # Elastic net (mix of lasso and ridge)
nfolds = 10         # 10-fold cross-validation
)
# Plot of Cross-Validation Results
# Optimal lambdas
lambda_min <- cv_elastic_net$lambda.min
lambda_1se <- cv_elastic_net$lambda.1se
# Plot deviance against log(lambda)
par(mfrow = c(1, 1))
plot(cv_elastic_net, main = "Cross-Validation for Elastic Net Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
cat("Optimal lambda (min):", lambda_min, "\n")
cat("Optimal lambda (1se):", lambda_1se, "\n")
# We find the lambda to the top left, meaning it could be further left:
# Define a custom lambda sequence (smaller values)
lambda_grid <- 10^seq(-10, 2, length = 100)  # Adjust range to include smaller lambda values
# Perform Cross-Validation with custom lambda grid
set.seed(123)
cv_elastic_net <- cv.glmnet(
x_train, y_train,
family = "binomial",
standarize = T,
alpha = 0.5,
lambda = lambda_grid,
nfolds = 10
)
# Optimal lambdas
lambda_min <- cv_elastic_net$lambda.min
lambda_1se <- cv_elastic_net$lambda.1se
# Plot the updated deviance vs log(lambda) with the extended search range
plot(cv_elastic_net, main = "Extended Cross-Validation for Elastic Net Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
### 3. Fit the Final Model with Optimal Lambda -----------------
elastic_net_model <- glmnet(
x_train, y_train,
family = "binomial",
standarize = T,
alpha = 0.5,        # Elastic net
lambda = lambda_min # Use lambda.min for final model
)
### 4. Evaluate Model ------------------------------------------
# Predicted probabilities
predicted_prob <- predict(elastic_net_model, s = lambda_min, newx = x_test, type = "response")
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Model evaluation metrics
evaluate_elastic_net <- function(predicted_class, predicted_prob, y_test) {
accuracy <- mean(predicted_class == y_test)
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
auc_value <- auc(roc(y_test, predicted_prob))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}
elastic_net_results <- evaluate_elastic_net(predicted_class, predicted_prob, y_test)
print(elastic_net_results)
### 5. Post-Estimation Plots -----------------------------------
# (1) ROC Curve and AUC
roc_curve <- roc(y_test, predicted_prob)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(elastic_net_results$auc, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# (2) Confusion Matrix Heatmap
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
# (3) Coefficient Plot
coef_data <- as.data.frame(as.matrix(coef(elastic_net_model, s = lambda_min)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Elastic Net Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
# (4) Calibration Curve
calibration_data <- data.frame(
Predicted = as.vector(predicted_prob),
Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
