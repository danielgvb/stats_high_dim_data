### 9.2) Train Model--------------------
svm_model <- svm(
default_90 ~ .,
data = train_data_scaled,
kernel = "radial",
cost = 1,
gamma = 0.1,
probability = TRUE # Train the model with probability enabled
)
# Step 1: Adjust target variable levels to valid R names
train_data_scaled$default_90 <- factor(train_data_scaled$default_90, levels = c(0, 1), labels = c("No", "Yes"))
test_data_scaled$default_90 <- factor(test_data_scaled$default_90, levels = c(0, 1), labels = c("No", "Yes"))
# Step 2: Define trainControl for cross-validation
train_control <- trainControl(
method = "cv",          # Cross-validation
number = 5,             # Number of folds
classProbs = TRUE,      # Compute class probabilities
verboseIter = TRUE      # Show progress during training
)
# Step 3: Define parameter grid for cost and gamma
tune_grid <- expand.grid(
C = c(0.1, 1, 10, 100),  # Values for cost
sigma = c(0.01, 0.1, 1)  # Values for gamma
)
# Step 4: Train the SVM model with grid search
svm_model <- train(
default_90 ~ .,           # Formula
data = train_data_scaled, # Training data
method = "svmRadial",     # Radial kernel SVM
tuneGrid = tune_grid,     # Hyperparameter grid
trControl = train_control # Cross-validation settings
)
# Step 5: View the best parameters and results
print(svm_model$bestTune)  # Best combination of cost and gamma
#sigma C
#4  0.01 1
print(svm_model)           # Full results
# Step 6: Make predictions on the test set
test_data_features <- test_data_scaled %>% select(-default_90)  # Exclude the target variable
svm_preds <- predict(svm_model, newdata = test_data_features)
### 9.4) Model Evaluation-----------------
# Make predictions with probabilities
svm_probs <- predict(svm_model, newdata = test_data_features, probability = TRUE)
# Extract the probabilities matrix
probs_matrix <- attr(svm_probs, "probabilities")
# Get the probability for class '1'
svm_class_1_probs <- probs_matrix[, "1"]  # Adjust "1" to match your class label
svm_metrics <- calculate_metrics(svm_class_1_probs, test_data_scaled$default_90)
svm_class_1_probs
svm_probs
# Get the probability for class '1'
svm_class_1_probs <- probs_matrix[, "Yes"]  # Adjust "1" to match your class label
svm_class_1_probs
### 9.4) Model Evaluation-----------------
# Make predictions with probabilities
svm_probs <- predict(svm_model, newdata = test_data_features, probability = TRUE)
svm_probs
# Step 4: Train the SVM model with grid search
svm_model <- train(
default_90 ~ .,           # Formula
data = train_data_scaled, # Training data
method = "svmRadial",     # Radial kernel SVM
tuneGrid = tune_grid,     # Hyperparameter grid
trControl = train_control, # Cross-validation settings
probability = TRUE # Train the model with probability enabled
)
# Step 5: View the best parameters and results
print(svm_model$bestTune)  # Best combination of cost and gamma
#sigma C
#4  0.01 1
print(svm_model)  # Full results
# Step 6: Make predictions on the test set
test_data_features <- test_data_scaled %>% select(-default_90)  # Exclude the target variable
svm_preds <- predict(svm_model, newdata = test_data_features)
svm_preds
### 9.4) Model Evaluation-----------------
# Make predictions with probabilities
svm_probs <- predict(svm_model, newdata = test_data_features, probability = TRUE)
svm_probs
# Extract the probabilities matrix
probs_matrix <- attr(svm_probs, "probabilities")
probs_matrix
# Step 7: Evaluate the model's performance
confusionMatrix(svm_preds, test_data_scaled$default_90)
confusionMatrix(svm_preds, test_data_scaled$default_90)
# Load Required Libraries --------------------------------
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC)
library(car)
library(gglasso)
library(doParallel)
library(mgcv) # for GAM
library(sparsepca) # for sparse pca
library(gridExtra)
library(ggthemes)
# Set Working Directory ---------------------------------
data_path <- "../data/data.xlsx"
# Import Data -------------------------------------------
data <- read_excel(data_path)
#Initial check of the data
summary(data)
# Ideally we should not check each column
# because in HD dataset we could have a very huge number of features,
# so maybe anyNA() could be more appropriate and faster
anyNA(data)
# check if there are sing-val col
constant_cols <- sapply(data, function(col) length(unique(col)) == 1)
names(data)[constant_cols]
# eventually remove them
data <- data[, !names(data) %in% "Clasificación Tipo Crédito"]
## Remove ID Variables
id_vars <- c("Código de Crédito", "ID Cliente", "No Pagaré Rotativo")
data <- data[, !names(data) %in% id_vars]
# Load Required Libraries --------------------------------
rm(list=ls())
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC)
library(car)
library(gglasso)
library(doParallel)
library(mgcv) # for GAM
library(sparsepca) # for sparse pca
library(gridExtra)
library(ggthemes)
# Set Working Directory ---------------------------------
data_path <- "../data/data.xlsx"
# Import Data -------------------------------------------
data <- read_excel(data_path)
#Initial check of the data
summary(data)
# Ideally we should not check each column
# because in HD dataset we could have a very huge number of features,
# so maybe anyNA() could be more appropriate and faster
anyNA(data)
# check if there are sing-val col
constant_cols <- sapply(data, function(col) length(unique(col)) == 1)
names(data)[constant_cols]
# eventually remove them
data <- data[, !names(data) %in% "Clasificación Tipo Crédito"]
# Create CO-debtor column
# Aggregate to count distinct 'ID Cliente' by 'No Pagaré Rotativo'
result <- aggregate(`ID Cliente` ~ `No Pagaré Rotativo`, data = data, FUN = function(x) length(unique(x)))
# Rename columns of result
colnames(result) <- c("No Pagaré Rotativo", "Distinct ID Cliente Count")
# View result max count by id
max(result$`Distinct ID Cliente Count`)
# mark if the credit has more than one ID.
# Compute the distinct ID Cliente count per No Pagaré Rotativo
distinct_counts <- aggregate(`ID Cliente` ~ `No Pagaré Rotativo`, data = data, FUN = function(x) length(unique(x)))
# Add a column indicating whether the count is greater than 1
distinct_counts$MoreThanOne <- as.numeric(distinct_counts$`ID Cliente` > 1)
# Merge this information back into the original dataframe
data <- merge(data, distinct_counts[, c("No Pagaré Rotativo", "MoreThanOne")], by = "No Pagaré Rotativo", all.x = TRUE)
colnames(data)
## Remove ID Variables
id_vars <- c("Código de Crédito", "ID Cliente", "No Pagaré Rotativo")
data <- data[, !names(data) %in% id_vars]
## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity", "credit_duration", "date_limit",
"dtf_approval_date", "fx_approval_date", "city_pop_2018", "default_90", "has_codebtor")
if (length(friendly_names) == ncol(data)) {
colnames(data) <- friendly_names
} else {
stop("Column name mismatch.")
}
# Load Required Libraries --------------------------------
rm(list=ls())
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC)
library(car)
library(gglasso)
library(doParallel)
library(mgcv) # for GAM
library(sparsepca) # for sparse pca
library(gridExtra)
library(ggthemes)
# Set Working Directory ---------------------------------
data_path <- "../data/data.xlsx"
# Import Data -------------------------------------------
data <- read_excel(data_path)
# Data Preprocessing ------------------------------------
#Initial check of the data
summary(data)
## Handle Missing Values
#na_counts <- colSums(is.na(data))
#print(na_counts)
# Ideally we should not check each column
# because in HD dataset we could have a very huge number of features,
# so maybe anyNA() could be more appropriate and faster
anyNA(data)
## Single-Value columns
## Remove Single-Value Columns
#single_value_vars <- c("Clasificación Tipo Crédito")
#data <- data[, !names(data) %in% single_value_vars]
# check if there are sing-val col
constant_cols <- sapply(data, function(col) length(unique(col)) == 1)
names(data)[constant_cols]
# eventually remove them
data <- data[, !names(data) %in% "Clasificación Tipo Crédito"]
# Create CO-debtor column
# Aggregate to count distinct 'ID Cliente' by 'No Pagaré Rotativo'
result <- aggregate(`ID Cliente` ~ `No Pagaré Rotativo`, data = data, FUN = function(x) length(unique(x)))
# Rename columns of result
colnames(result) <- c("No Pagaré Rotativo", "Distinct ID Cliente Count")
# View result max count by id
max(result$`Distinct ID Cliente Count`)
# so there is some credits with more than one client associated
# mark if the credit has more than one ID.
# Compute the distinct ID Cliente count per No Pagaré Rotativo
distinct_counts <- aggregate(`ID Cliente` ~ `No Pagaré Rotativo`, data = data, FUN = function(x) length(unique(x)))
# Add a column indicating whether the count is greater than 1
distinct_counts$MoreThanOne <- as.numeric(distinct_counts$`ID Cliente` > 1)
# Merge this information back into the original dataframe
data <- merge(data, distinct_counts[, c("No Pagaré Rotativo", "MoreThanOne")], by = "No Pagaré Rotativo", all.x = TRUE)
colnames(data)
## Remove ID Variables
id_vars <- c("Código de Crédito", "ID Cliente", "No Pagaré Rotativo")
data <- data[, !names(data) %in% id_vars]
## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity", "credit_duration", "date_limit",
"dtf_approval_date", "fx_approval_date", "city_pop_2018","datacredito","default_90", "has_codebtor")
if (length(friendly_names) == ncol(data)) {
## Rename Columns for Clarity
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity", "credit_duration", "date_limit",
"dtf_approval_date", "fx_approval_date", "city_pop_2018","datacredito","default_90", "has_codebtor")
if (length(friendly_names) == ncol(data)) {
colnames(data) <- friendly_names
} else {
stop("Column name mismatch.")
}
#REMOVE AGE because of data leakage--------------
data <- data[, !names(data) %in% c("age")]
## DateTime variables-----------
# Create Derived Variables
data <- data %>%
mutate(
time_difference_days = as.numeric(difftime(as.Date(date_limit), as.Date(date_approval), units = "days"))
)
# First create year, month, day, weekday
# Extract features from the date-time variables
# date approval
data$m_date_approval <- format(data$date_approval, "%m")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$m_date_limit <- format(data$date_limit, "%m")
data$wd_date_limit <- weekdays(data$date_limit)
# Remove date_approval and date_limit:
# I think they're not useful now that they have been used to create a new variable
data <- data[, !(names(data) %in% c("date_approval", "date_limit"))]
# -> I suggest to transform it in binary variable 0/1 indicating if a client has overdue the credit in 30+ days
# --> assuming that 30+ days overdue is more likely to progress to 90+ days overdue rather than who's currently at 0 days
data <- data %>%
mutate(
capital_due30_binary = ifelse(capital_due30 > 0, 1, 0)
)
# Remove capital_due30 in order to keep only capital_due30_binary
data <- data[, !names(data) %in% "capital_due30"]
# Move "default_90" to the last column
data <- data[, c(setdiff(names(data), "default_90"), "default_90")]
# income_group has numbers indicating groups, so maybe it's more appropriate
# considering it as an ordinal categorical variable instead of a numerical variable
data$income_group <- factor(data$income_group, ordered = TRUE)
numeric_data <- data %>% select(where(is.numeric))
skim(numeric_data)
### Histograms
for (col_name in colnames(numeric_data)) {
hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
xlab = col_name, col = "lightblue", border = "black")
}
### Box Plots---------------
for (col_name in colnames(numeric_data)) {
print(
ggplot(data, aes(x = factor(default_90), y = .data[[col_name]], fill = factor(default_90))) +
geom_boxplot(alpha = 0.7) +  # Add transparency to boxplots
labs(
title = paste("Boxplot of", col_name, "by Target"),
x = "Target",
y = col_name,
fill = "Target"
) +
theme_economist() +  # Apply The Economist theme
scale_fill_economist() +  # Use The Economist fill palette
theme(
axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels if needed
axis.title.x = element_text(margin = margin(t = 10))  # Add margin to x-axis title
)
)
}
### Density Plots--------------
for (col_name in colnames(numeric_data)) {
print(
ggplot(data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +  # Set transparency for overlapping densities
labs(
title = paste("Density Plot of", col_name),
x = col_name,
fill = "Target"
) +
theme_economist() +  # Apply The Economist theme
scale_fill_economist() +  # Use Economist palette for the fill aesthetic
theme(
axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for clarity
axis.title.x = element_text(margin = margin(t = 10))  # Add margin to x-axis title
)
)
}
### Correlation Matrix -------------------
cor_matrix <- cor(numeric_data, use = "complete.obs")
melted_cor_matrix <- melt(cor_matrix)
melted_cor_matrix <- melted_cor_matrix[as.numeric(melted_cor_matrix$Var1) > as.numeric(melted_cor_matrix$Var2), ]
ggplot(data = melted_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
theme_minimal() +
coord_fixed() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid = element_blank()  # in order to remove the grid
) +
scale_x_discrete(limits = unique(melted_cor_matrix$Var1)) +
scale_y_discrete(limits = unique(melted_cor_matrix$Var2))
cor_matrix
non_numeric_data <- data %>% select(where(~ !is.numeric(.)))
unique_counts <- sapply(non_numeric_data, function(x) length(unique(x)))
mode_values <- sapply(non_numeric_data, function(x) names(which.max(table(x))))
print(mode_values)
# Create a vector to store p-values
p_values <- c()
# Perform Chi-square tests and collect p-values
for (col_name in colnames(non_numeric_data)) {
contingency_table <- table(non_numeric_data[[col_name]], data$default_90)
chi2_result <- chisq.test(contingency_table)
# Store the p-value
p_values <- c(p_values, chi2_result$p.value)
}
contingency_table
# Create a data frame for plotting
chi2_results_df <- data.frame(
Variable = colnames(non_numeric_data),
P_value = p_values
)
# Plot the p-values
ggplot(chi2_results_df, aes(x = reorder(Variable, -P_value), y = P_value)) +
geom_bar(stat = "identity", fill = "skyblue") +
geom_hline(yintercept = 0.05, color = "red", linetype = "dashed") +  # Significance threshold
labs(title = "P-values from Chi-square Tests for Non-numeric Variables vs default_90",
x = "Variable",
y = "P-value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Transformations ---------------------------------------
# Applying log transformation only to too much skewed features bc for the others could be sufficient the scaling
data$contributions_balance <- log1p(data$contributions_balance)
data$credit_limit <- log1p(data$credit_limit)
data$capital_balance <- log1p(data$capital_balance)
data$installment <- log1p(data$installment)
data$time_difference_days <- log1p(data$time_difference_days)
### Histograms ----------------
for (col_name in colnames(numeric_data)) {
hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
xlab = col_name, col = "lightblue", border = "black")
}
### Box Plots---------------
for (col_name in colnames(numeric_data)) {
print(
ggplot(data, aes(x = factor(default_90), y = .data[[col_name]], fill = factor(default_90))) +
geom_boxplot(alpha = 0.7) +  # Add transparency to boxplots
labs(
title = paste("Boxplot of", col_name, "by Target"),
x = "Target",
y = col_name,
fill = "Target"
) +
theme_economist() +  # Apply The Economist theme
scale_fill_economist() +  # Use The Economist fill palette
theme(
axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels if needed
axis.title.x = element_text(margin = margin(t = 10))  # Add margin to x-axis title
)
)
}
### Density Plots--------------
for (col_name in colnames(numeric_data)) {
print(
ggplot(data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +  # Set transparency for overlapping densities
labs(
title = paste("Density Plot of", col_name),
x = col_name,
fill = "Target"
) +
theme_economist() +  # Apply The Economist theme
scale_fill_economist() +  # Use Economist palette for the fill aesthetic
theme(
axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels for clarity
axis.title.x = element_text(margin = margin(t = 10))  # Add margin to x-axis title
)
)
}
# Train-Test Split --------------------------------------
set.seed(123)
split <- sample.split(data$default_90, SplitRatio = 0.7)
train_data <- subset(data, split == TRUE)
test_data <- subset(data, split == FALSE)
# do not consider binary variables for scaling:
binary_vars <- names(data)[sapply(data, function(x) all(x %in% c(0, 1)))]
binary_vars
# Exclude binary variables and the target from the numeric_data dataframe
vars_to_scale <- numeric_data[, !(names(numeric_data) %in% binary_vars)]
## Extract column names from vars_to_scale
vars_to_scale_names <- colnames(vars_to_scale)
## Mean and SD in train_data scaling vars_to_scale
mean_train <- apply(train_data[vars_to_scale_names], 2, mean)
sd_train <- apply(train_data[vars_to_scale_names], 2, sd)
# Scale the training set
train_data_scaled <- train_data
train_data_scaled[vars_to_scale_names] <- sweep(train_data[vars_to_scale_names], 2, mean_train, "-")
train_data_scaled[vars_to_scale_names] <- sweep(train_data_scaled[vars_to_scale_names], 2, sd_train, "/")
# Scale the test set
test_data_scaled <- test_data
test_data_scaled[vars_to_scale_names] <- sweep(test_data[vars_to_scale_names], 2, mean_train, "-")
test_data_scaled[vars_to_scale_names] <- sweep(test_data_scaled[vars_to_scale_names], 2, sd_train, "/")
# Check scaling Z-score -> mean = 0; sd = 1
summary(train_data_scaled[vars_to_scale_names])
apply(train_data_scaled[vars_to_scale_names], 2, sd)
table(train_data_scaled$default_90)
# oversampling of minority class:
library(ROSE)
train_data_balanced <- ovun.sample(default_90 ~ ., data = train_data_scaled, method = "over")$data
table(train_data_balanced$default_90)
## Logistic Regression (Balanced Training Data) ----------------
logistic_model_balanced <- glm(default_90 ~ ., data = train_data_balanced, family = binomial)
summary(logistic_model_balanced)
# Load Required Libraries --------------------------------
rm(list=ls())
library(readxl)
library(ggplot2)
library(caret)
library(glmnet)
library(skimr)
library(reshape2)
library(caTools)
library(ResourceSelection)
library(pROC) # For AUC Curve
library(PRROC)    # For Precision-Recall Curve
library(MASS)  # For stepwise regression functions
library(car)
library(gglasso)
library(doParallel)
library(mgcv) # for GAM
library(sparsepca) # for sparse pca
library(e1071) # for SVM
library(dplyr)
library(randomForest)
library(xgboost)
library(Matrix)
# Set Working Directory ---------------------------------
data_path <- "../data/data.xlsx"
# Import Data -------------------------------------------
data <- read_excel(data_path)
# Check the type of each column and count numeric and categorical
numeric_columns <- sum(sapply(data, function(col) is.numeric(col) || inherits(col, "Date")))
categorical_columns <- sum(sapply(data, function(col) is.factor(col) || is.character(col)))
# Print the counts
cat("Number of numeric (including date) columns:", numeric_columns, "\n")
cat("Number of categorical columns:", categorical_columns, "\n")
length(colnames(data))
29-12-15
str(data)
numeric_columns <- sum(sapply(data, function(col) is.numeric(col) || inherits(col, "Date") || inherits(col, "POSIXct")))
categorical_columns <- sum(sapply(data, function(col) is.factor(col) || is.character(col)))
# Print the counts
cat("Number of numeric (including date and POSIXct) columns:", numeric_columns, "\n")
cat("Number of categorical columns:", categorical_columns, "\n")
