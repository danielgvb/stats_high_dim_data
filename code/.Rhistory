# Train Set
train_data_no_target <- train_data[, !colnames(train_data) %in% "default_90"]
numeric_data_train <- train_data_no_target[sapply(train_data_no_target, is.numeric)]
non_numeric_data_train <- train_data_no_target[, !sapply(train_data_no_target, is.numeric)]
group_vector_train <- c()  # Stores group IDs
dummy_list_train <- list()  # Stores dummy variables
group_id <- 1
# Numeric variables (each variable is its own group)
group_vector_train <- c(group_vector_train, rep(group_id, ncol(numeric_data_train)))
group_id <- group_id + 1
# Factor variables (each factor's dummies are a group)
for (col_name in colnames(non_numeric_data_train)) {
# Create dummy variables, dropping the first level
dummies <- model.matrix(as.formula(paste("~", col_name)), data = train_data_no_target)[, -1]
# Ensure dummies is treated as a matrix
dummies <- as.matrix(dummies)
# Check if the resulting dummies matrix has at least one column
if (!is.null(ncol(dummies)) && ncol(dummies) > 0) {
# Store the dummies in the list
dummy_list_train[[col_name]] <- dummies
# Add group markers for the current set of dummy variables
group_vector_train <- c(group_vector_train, rep(group_id, ncol(dummies)))
# Increment the group ID for the next factor
group_id <- group_id + 1
} else {
# Handle cases where no dummy variables are created (e.g., single-level factors)
warning(paste("No dummy variables created for", col_name, "as it has only one level."))
}
}
# Combine numeric and dummy data for the train set
dummy_data_train <- do.call(cbind, dummy_list_train)
combined_data_train <- cbind(numeric_data_train, dummy_data_train)
X_train <- as.matrix(combined_data_train)
y_train <- ifelse(train_data$default_90 == 1, 1, -1)  # Convert target to {-1, 1}
# Test Set
test_data_no_target <- test_data[, !colnames(test_data) %in% "default_90"]
numeric_data_test <- test_data_no_target[sapply(test_data_no_target, is.numeric)]
non_numeric_data_test <- test_data_no_target[, !sapply(test_data_no_target, is.numeric)]
group_vector_test <- c()
dummy_list_test <- list()
group_id <- 1
# Numeric variables
group_vector_test <- c(group_vector_test, rep(group_id, ncol(numeric_data_test)))
group_id <- group_id + 1
# Factor variables
for (col_name in colnames(non_numeric_data_test)) {
dummies <- model.matrix(as.formula(paste("~", col_name)), data = test_data_no_target)[, -1]
dummies <- as.matrix(dummies)
if (!is.null(ncol(dummies)) && ncol(dummies) > 0) {
dummy_list_test[[col_name]] <- dummies
group_vector_test <- c(group_vector_test, rep(group_id, ncol(dummies)))
group_id <- group_id + 1
} else {
warning(paste("No dummy variables created for", col_name, "as it has only one level."))
}
}
# Combine numeric and dummy data for the test set
dummy_data_test <- do.call(cbind, dummy_list_test)
combined_data_test <- cbind(numeric_data_test, dummy_data_test)
X_test <- as.matrix(combined_data_test)
y_test <- ifelse(test_data$default_90 == 1, 1, -1)  # Convert target to {-1, 1}
### 2. Standardize Predictors ---------------------------------
X_train_scaled <- scale(X_train)
X_test_scaled <- scale(X_test)
# Set up parallel processing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
# Fit group lasso model
fit_gglasso <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
nlambda = 50
)
# Stop parallel processing
stopCluster(cl)
# Plot Group Lasso Fit
plot(fit_gglasso, main = "Group Lasso Coefficients Across Lambda")
evaluate_gglasso_model <- function(model, test_x, test_y, threshold = 0.5) {
# Get the smallest lambda value
lambda_min <- min(model$lambda)
# Find the index of the smallest lambda
best_lambda_index <- which(model$lambda == lambda_min)
# Predict log-odds for the smallest lambda
log_odds <- predict(model, newx = test_x, type = "link")[, best_lambda_index]
# Convert log-odds to probabilities using the sigmoid function
predicted_prob <- 1 / (1 + exp(-log_odds))
# Convert probabilities to binary predictions using the threshold
predicted_class <- ifelse(predicted_prob > threshold, 1, -1)
# Ensure test_y is properly aligned
if (length(predicted_class) != length(test_y)) {
stop("Mismatch between predicted values and test_y. Check input dimensions.")
}
# Calculate accuracy
accuracy <- mean(predicted_class == test_y)
# Create confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_y)
# Precision, recall, and F1 score
true_positive <- ifelse("1" %in% rownames(conf_matrix) & "1" %in% colnames(conf_matrix),
conf_matrix["1", "1"], 0)
false_positive <- ifelse("1" %in% rownames(conf_matrix) & "-1" %in% colnames(conf_matrix),
conf_matrix["1", "-1"], 0)
false_negative <- ifelse("-1" %in% rownames(conf_matrix) & "1" %in% colnames(conf_matrix),
conf_matrix["-1", "1"], 0)
precision <- ifelse(true_positive + false_positive > 0,
true_positive / (true_positive + false_positive), 0)
recall <- ifelse(true_positive + false_negative > 0,
true_positive / (true_positive + false_negative), 0)
f1_score <- ifelse(precision + recall > 0,
2 * (precision * recall) / (precision + recall), 0)
# Return evaluation metrics
return(list(
accuracy = round(accuracy, 4),
f1_score = round(f1_score, 4),
confusion_matrix = conf_matrix
))
}
results_gglasso <- evaluate_gglasso_model(
model = fit_gglasso,
test_x = X_test_scaled,
test_y = y_test
)
# Print results
print(paste("Accuracy:", results_gglasso$accuracy))
print(paste("F1 Score:", results_gglasso$f1_score))
print("Confusion Matrix:")
print(results_gglasso$confusion_matrix)
# Extract coefficients for the optimal lambda
lambda_optimal <- min(fit_gglasso$lambda)  # Smallest lambda
coefficients <- coef(fit_gglasso, s = lambda_optimal)
# Convert to a data frame
coef_data <- as.data.frame(as.matrix(coefficients))
coef_data$Variable <- rownames(coef_data)
rownames(coef_data) <- NULL
# Rename the coefficient column (default name is "s1" or similar)
colnames(coef_data)[1] <- "Coefficient"
# Filter for non-zero coefficients
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Group Lasso Coefficients", x = "Variable", y = "Coefficient") +
theme_minimal()
# Predict log-odds for the smallest lambda
lambda_min <- min(fit_gglasso$lambda)
# Find the index of the smallest lambda
best_lambda_index <- which(fit_gglasso$lambda == lambda_min)
log_odds <- predict(fit_gglasso, newx = X_test_scaled, type = "link")[, best_lambda_index]
# Convert log-odds to probabilities using the sigmoid function
predicted_prob <- 1 / (1 + exp(-log_odds))
calibration_data <- data.frame(
Predicted = as.vector(predicted_prob),
Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = -1, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Group Lasso)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
## Group Lasso with cross validation----------------
# Set up k-fold cross-validation
k <- 10  # Number of folds
folds <- createFolds(y_train, k = k, list = TRUE)
# Prepare storage for cross-validation results
cv_results <- matrix(0, nrow = length(fit_gglasso$lambda), ncol = k)
# Set up parallel processing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
# Perform cross-validation
for (fold_idx in 1:k) {
# Split data into training and validation sets for this fold
val_idx <- folds[[fold_idx]]
X_train_cv <- X_train_scaled[-val_idx, ]
y_train_cv <- y_train[-val_idx]
X_val_cv <- X_train_scaled[val_idx, ]
y_val_cv <- y_train[val_idx]
# Fit group lasso on training data
fit_cv <- gglasso(
x = X_train_cv,
y = y_train_cv,
group = group_vector_train,
loss = "logit",
nlambda = 50
)
# Evaluate on validation data
for (lambda_idx in 1:length(fit_cv$lambda)) {
# Predict for this lambda
log_odds <- predict(fit_cv, newx = X_val_cv, type = "link")[, lambda_idx]
predicted_prob <- 1 / (1 + exp(-log_odds))
predicted_class <- ifelse(predicted_prob > 0.5, 1, -1)
# Store validation accuracy
cv_results[lambda_idx, fold_idx] <- mean(predicted_class == y_val_cv)
}
}
# Stop parallel processing
stopCluster(cl)
# Average accuracy across folds for each lambda
cv_mean_accuracy <- rowMeans(cv_results)
# Identify the best lambda (maximum accuracy)
best_lambda_idx <- which.max(cv_mean_accuracy)
best_lambda <- fit_gglasso$lambda[best_lambda_idx]
# Print best lambda
cat("Best lambda:", best_lambda, "\n")
# Plot lambda min and 1se
plot(fit_gglasso)
# Refit the model on the full training data with the best lambda
final_fit <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
lambda = best_lambda
)
# Evaluate the final model on the test set
log_odds_test <- predict(final_fit, newx = X_test_scaled, type = "link")
predicted_prob_test <- 1 / (1 + exp(-log_odds_test))
predicted_class_test <- ifelse(predicted_prob_test > 0.5, 1, -1)
accuracy_test <- mean(predicted_class_test == y_test)
cat("Test set accuracy with best lambda:", accuracy_test, "\n")
# Evaluate the final cross-validated model on the test set
results_cv <- evaluate_gglasso_model(
model = final_fit,
test_x = X_test_scaled,
test_y = y_test
)
# Print results
print(paste("Accuracy (CV):", results_cv$accuracy))
print(paste("F1 Score (CV):", results_cv$f1_score))
print("Confusion Matrix (CV):")
print(results_cv$confusion_matrix)
# (1) Coefficient Plot for Cross-Validated Model
lambda_cv <- best_lambda
coefficients_cv <- coef(final_fit, s = lambda_cv)
# Convert to a data frame
coef_data_cv <- as.data.frame(as.matrix(coefficients_cv))
coef_data_cv$Variable <- rownames(coef_data_cv)
rownames(coef_data_cv) <- NULL
# Rename the coefficient column (default name is "s1" or similar)
colnames(coef_data_cv)[1] <- "Coefficient"
# Filter for non-zero coefficients
coef_data_cv <- coef_data_cv %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data_cv, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Group Lasso Coefficients (CV Model)", x = "Variable", y = "Coefficient") +
theme_minimal()
# (2) Calibration Curve for Cross-Validated Model
y_test_binary <- ifelse(y_test == -1, 0, 1)
log_odds_cv <- predict(final_fit, newx = X_test_scaled, type = "link")
# Set up parallel processing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
# Fit group lasso model
fit_gglasso <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
nlambda = 50
)
# Stop parallel processing
stopCluster(cl)
# Plot Group Lasso Fit
plot(fit_gglasso, main = "Group Lasso Coefficients Across Lambda")
### 4. Perform Cross-Validation for Group Lasso -----------------
set.seed(123)
### 4. Perform Cross-Validation for Group Lasso -----------------
set.seed(123)
cv_fit_gglasso <- cv.gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
nfolds = 10
)
# Optimal lambda
lambda_min_gl <- cv_fit_gglasso$lambda.min
cat("Optimal lambda (min):", lambda_min_gl, "\n")
# Plot Cross-Validation Results for Group Lasso
plot(cv_fit_gglasso, main = "Cross-Validation for Group Lasso")
# Optimal lambda
lambda_min_gl <- cv_fit_gglasso$lambda.min
lambda_1se_gl <- cv_fit_gglasso$lambda.1se
cat("Optimal lambda (min):", lambda_min_gl, "\n")
cat("Optimal lambda (1se):", lambda_1se_gl, "\n")
# Plot Cross-Validation Results for Group Lasso
plot(cv_fit_gglasso, main = "Cross-Validation for Group Lasso")
abline(v = log(lambda_min_gl), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se_gl), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
# Plot Cross-Validation Results for Group Lasso
plot(cv_fit_gglasso, main = "Cross-Validation for Group Lasso")
# Optimal lambda
lambda_min_gl <- cv_fit_gglasso$lambda.min
lambda_1se_gl <- cv_fit_gglasso$lambda.1se
cat("Optimal lambda (min):", lambda_min_gl, "\n")
cat("Optimal lambda (1se):", lambda_1se_gl, "\n")
# Plot Cross-Validation Results for Group Lasso
plot(cv_fit_gglasso, main = "Cross-Validation for Group Lasso")
abline(v = log(lambda_min_gl), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se_gl), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
### 5. Fit the Final Group Lasso Model -------------------------
group_lasso_model <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
lambda = lambda_min_gl
)
# Optimal lambda
lambda_min_gl <- cv_fit_gglasso$lambda.min
lambda_1se_gl <- cv_fit_gglasso$lambda.1se
cat("Optimal lambda (min):", lambda_min_gl, "\n")
cat("Optimal lambda (1se):", lambda_1se_gl, "\n")
# Plot Cross-Validation Results for Group Lasso
plot(cv_fit_gglasso, main = "Cross-Validation for Group Lasso")
abline(v = log(lambda_min_gl), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se_gl), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
### 5. Fit the Final Group Lasso Model -------------------------
group_lasso_model <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
lambda = lambda_min_gl
)
### 6. Evaluate Group Lasso Model ------------------------------
# Predicted probabilities
log_odds_gl <- predict(group_lasso_model, newx = X_test_scaled, type = "link")
predicted_prob_gl <- 1 / (1 + exp(-log_odds_gl))
# Predicted classes
predicted_class_gl <- ifelse(predicted_prob_gl > 0.5, 1, 0)
# Model evaluation metrics
evaluate_group_lasso <- function(predicted_class, predicted_prob, y_test) {
accuracy <- mean(predicted_class == y_test)
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
auc_value <- auc(roc(y_test, predicted_prob))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}
group_lasso_results <- evaluate_group_lasso(predicted_class_gl, predicted_prob_gl, y_test)
print(group_lasso_results)
log_odds_gl
predicted_prob_gl
predicted_class_gl
# Model evaluation metrics
evaluate_group_lasso <- function(predicted_class, predicted_prob, y_test) {
accuracy <- mean(predicted_class == y_test)
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
auc_value <- auc(roc(y_test, predicted_prob))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}
group_lasso_results <- evaluate_group_lasso(predicted_class_gl, predicted_prob_gl, y_test)
y_test
# Model evaluation metrics
evaluate_group_lasso <- function(predicted_class, predicted_prob, y_test) {
accuracy <- mean(predicted_class == y_test)
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
auc_value <- auc(roc(y_test, predicted_prob))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}
group_lasso_results <- evaluate_group_lasso(predicted_class_gl, predicted_prob_gl, y_test)
# Predicted classes
predicted_class_gl <- ifelse(predicted_prob_gl > 0.5, 1, -1)
# Model evaluation metrics
evaluate_group_lasso <- function(predicted_class, predicted_prob, y_test) {
accuracy <- mean(predicted_class == y_test)
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
auc_value <- auc(roc(y_test, predicted_prob))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}
group_lasso_results <- evaluate_group_lasso(predicted_class_gl, predicted_prob_gl, y_test)
print(group_lasso_results)
# (1) ROC Curve and AUC
roc_curve_gl <- roc(y_test, predicted_prob_gl)
plot(roc_curve_gl, col = "blue", main = paste("ROC Curve (AUC =", round(group_lasso_results$auc, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# (2) Confusion Matrix Heatmap
conf_matrix_gl <- table(Predicted = predicted_class_gl, Actual = y_test)
ggplot(as.data.frame(conf_matrix_gl), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap (Group Lasso)", x = "Actual", y = "Predicted") +
theme_minimal()
# (3) Coefficient Plot
coef_data_gl <- as.data.frame(as.matrix(coef(group_lasso_model)))
coef_data_gl$Variable <- rownames(coef_data_gl)
colnames(coef_data_gl) <- c("Coefficient", "Variable")
coef_data_gl <- coef_data_gl %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data_gl, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Group Lasso Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
# (4) Calibration Curve
calibration_data_gl <- data.frame(
Predicted = as.vector(predicted_prob_gl),
Observed = y_test
)
calibration_data_gl$Bin <- cut(calibration_data_gl$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve_gl <- calibration_data_gl %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve_gl, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Group Lasso)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
ggplot(calibration_curve_gl, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = -1, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Group Lasso)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
ggplot(calibration_curve_gl, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 2, intercept = -1, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Group Lasso)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
## GAM Model----------------
# Using most relevant poly order (`s()` indicates the poly order for each predictor)
gam_model <- gam(default_90 ~  + s(credit_limit) + s(capital_balance) + s(days_due) + gender + income_group,
data = train_data,
family = binomial)
# Summary of GAM Model
summary(gam_model)
## GAM Model----------------
# Using most relevant poly order (`s()` indicates the poly order for each predictor)
gam_model <- gam(default_90 ~ s(age) + s(credit_limit) + s(capital_balance) + s(days_due) + gender + income_group,
data = train_data,
family = binomial)
# Summary of GAM Model
summary(gam_model)
# Evaluate GAM Model
evaluate_gam_model <- function(model, test_data, target_col) {
predicted_prob <- predict(model, newdata = test_data, type = "response")
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
accuracy <- mean(predicted_class == test_data[[target_col]])
confusion <- confusionMatrix(factor(predicted_class), factor(test_data[[target_col]]))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score))
}
gam_results <- evaluate_gam_model(gam_model, test_data, "default_90")
print(gam_results)
#### 1. ROC Curve and AUC ------------------------------------
# Predicted probabilities
predicted_prob <- predict(gam_model, newdata = test_data, type = "response")
# ROC and AUC
roc_curve <- roc(test_data$default_90, predicted_prob)
auc_value <- auc(roc_curve)
# Plot ROC Curve
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### 2. Confusion Matrix Heatmap -----------------------------
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
#### 3. Calibration Curve ------------------------------------
# Bin predicted probabilities
calibration_data <- data.frame(
Predicted = predicted_prob,
Observed = test_data$default_90
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
# Mean predicted and observed probabilities by bin
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
# Plot calibration curve
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
### Apply Sparse PCA to Train Data ------------------------------
set.seed(123)
# Center the data and check the scale parametrization-
k <- 10  # Number of components to consider initially
fit <- spca(X_train, k = k, scale = TRUE, verbose = FALSE)
