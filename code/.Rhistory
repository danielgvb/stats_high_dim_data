# legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
#
#
# # Lambda is still max left, there is something off with the model
# Optimal lambda
lambda_optimal <- cv_lasso$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")
### 3. Fit the Final Model with Optimal Lambda -----------------
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = lambda_optimal)
### Model Evaluation---------------
predicted_probs_l <- predict(lasso_model, s = lambda_optimal, newx = x_test, type = "response")
lasso_metrics <- calculate_metrics(predicted_probs_l, test_data$default_90, 0.15)
lasso_metrics
# True labels
true_labels <- y_test
#### 1. ROC Curve and AUC for Lasso Model----------
roc_curve <- roc(true_labels, predicted_probs_l)
auc_value <- auc(roc_curve)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR side by side
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### 2. Precision-Recall (PR) Curve for Lasso Model ------
# Generate PR curve
pr_curve <- pr.curve(scores.class0 = predicted_probs_l[true_labels == 1],
scores.class1 = predicted_probs_l[true_labels == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
### Fine Tune Threshold--------
# Define a function to evaluate metrics at different thresholds
# Define a range of thresholds to evaluate
thresholds <- seq(0, 1, by = 0.05)  # Example grid of thresholds
# Optimize threshold
threshold_results <- find_optimal_threshold(predicted_probs_l, true_labels, thresholds)
threshold_results
plot_metrics(threshold_results)
#### (2) Confusion Matrix Heatmap------------
predicted_class <- ifelse(predicted_prob > 0.15, 1, 0) # using optimal threshold
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
#### (3) Coefficient Plot ----------------------------------------
coef_data <- as.data.frame(as.matrix(coef(lasso_model, s = lambda_optimal)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Lasso Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
# get lasso vars to use in GAM
vars_lasso <- coef_data$Variable
print(vars_lasso)
## Elastic Net Logistic Regression--------------
### 1. Prepare Data for glmnet --------------------------------
### 2. Perform Cross-Validation for Elastic Net Logistic Regression --
# Elastic net uses `alpha` to mix lasso (alpha = 1) and ridge (alpha = 0)
set.seed(123)
cv_elastic_net <- cv.glmnet(
x_train, y_train,
standarize = T,
family = "binomial",
alpha = 0.5,        # Elastic net (mix of lasso and ridge)
nfolds = 10         # 10-fold cross-validation
)
# Plot of Cross-Validation Results
# Optimal lambdas
lambda_min <- cv_elastic_net$lambda.min
lambda_1se <- cv_elastic_net$lambda.1se
# Plot deviance against log(lambda)
par(mfrow = c(1, 1))
plot(cv_elastic_net, main = "Cross-Validation for Elastic Net Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
cat("Optimal lambda (min):", lambda_min, "\n")
cat("Optimal lambda (1se):", lambda_1se, "\n")
### 3. Fit the Final Model with Optimal Lambda -----------------
elastic_net_model <- glmnet(
x_train, y_train,
family = "binomial",
standarize = T,
alpha = 0.5,        # Elastic net
lambda = lambda_min # Use lambda.min for final model
)
### 4. Evaluate Model ------------------------------------------
# Predicted probabilities
predicted_probs_en <- predict(elastic_net_model, s = lambda_min, newx = x_test, type = "response")
### Model Evaluation---------------
en_metrics <- calculate_metrics(predicted_probs_en, test_data$default_90, 0.3)
en_metrics
# Optimize threshold
# Optimize threshold
threshold_results_en <- find_optimal_threshold(predicted_probs_en, true_labels, thresholds)
threshold_results_en
plot_metrics(threshold_results_en)
# use 0.15 aswell
### 5. Post-Estimation Plots -----------------------------------
#### (1) ROC Curve and AUC-------------
roc_curve <- roc(true_labels, predicted_probs_en)
auc_value <- auc(roc_curve)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR side by side
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### (2) Precision-Recall (PR) Curve -------------
# Generate PR curve
pr_curve <- pr.curve(scores.class0 = predicted_probs_en[true_labels == 1],
scores.class1 = predicted_probs_en[true_labels == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
# Happens like the prior model, so adjust the threshold
#### (2) Confusion Matrix Heatmap-------------
predicted_class <- ifelse(predicted_probs_en > 0.25, 1, 0) # using optimal threshold
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
# marginally better, with the adjusted threshold
#### (3) Coefficient Plot------------
coef_data <- as.data.frame(as.matrix(coef(elastic_net_model, s = lambda_min)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Elastic Net Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
## Group Lasso Logistic Regression--------------
### 1. Prepare Data for Group Lasso -------------------------
# Define groups for numeric and factor variables
# Train Set
train_data_no_target <- train_data[, !colnames(train_data) %in% "default_90"]
numeric_data_train <- train_data_no_target[sapply(train_data_no_target, is.numeric)]
non_numeric_data_train <- train_data_no_target[, !sapply(train_data_no_target, is.numeric)]
group_vector_train <- c()  # Stores group IDs
dummy_list_train <- list()  # Stores dummy variables
group_id <- 1
# Numeric variables (each variable is its own group)
group_vector_train <- c(group_vector_train, seq(group_id, length.out = ncol(numeric_data_train)))
group_id <- max(group_vector_train) + 1
# Factor variables (each factor's dummies are a group)
for (col_name in colnames(non_numeric_data_train)) {
# Create dummy variables, dropping the first level
dummies <- model.matrix(as.formula(paste("~", col_name)), data = train_data_no_target)[, -1]
# Ensure dummies is treated as a matrix
dummies <- as.matrix(dummies)
# Check if the resulting dummies matrix has at least one column
if (!is.null(ncol(dummies)) && ncol(dummies) > 0) {
# Store the dummies in the list
dummy_list_train[[col_name]] <- dummies
# Add group markers for the current set of dummy variables
group_vector_train <- c(group_vector_train, rep(group_id, ncol(dummies)))
# Increment the group ID for the next factor
group_id <- group_id + 1
} else {
# Handle cases where no dummy variables are created (e.g., single-level factors)
warning(paste("No dummy variables created for", col_name, "as it has only one level."))
}
}
# Combine numeric and dummy data for the train set
dummy_data_train <- do.call(cbind, dummy_list_train)
combined_data_train <- cbind(numeric_data_train, dummy_data_train)
X_train <- as.matrix(combined_data_train)
y_train <- ifelse(train_data$default_90 == 1, 1, -1)  # Convert target to {-1, 1}
# Test Set
test_data_no_target <- test_data[, !colnames(test_data) %in% "default_90"]
numeric_data_test <- test_data_no_target[sapply(test_data_no_target, is.numeric)]
non_numeric_data_test <- test_data_no_target[, !sapply(test_data_no_target, is.numeric)]
group_vector_test <- c()
dummy_list_test <- list()
group_id <- 1
# Numeric variables
group_vector_test <- c(group_vector_test, seq(group_id, length.out = ncol(numeric_data_test)))
group_id <- max(group_vector_test) + 1
# Factor variables
for (col_name in colnames(non_numeric_data_test)) {
dummies <- model.matrix(as.formula(paste("~", col_name)), data = test_data_no_target)[, -1]
dummies <- as.matrix(dummies)
if (!is.null(ncol(dummies)) && ncol(dummies) > 0) {
dummy_list_test[[col_name]] <- dummies
group_vector_test <- c(group_vector_test, rep(group_id, ncol(dummies)))
group_id <- group_id + 1
} else {
warning(paste("No dummy variables created for", col_name, "as it has only one level."))
}
}
# Combine numeric and dummy data for the test set
dummy_data_test <- do.call(cbind, dummy_list_test)
combined_data_test <- cbind(numeric_data_test, dummy_data_test)
X_test <- as.matrix(combined_data_test)
y_test <- ifelse(test_data$default_90 == 1, 1, -1)  # Convert target to {-1, 1}
### 2. Standardize Predictors ---------------------------------
X_train_scaled <- scale(X_train)
X_test_scaled <- scale(X_test)
### 3. Fit the Group Lasso Model ------------------------------
# Set up parallel processing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
# Fit group lasso model
fit_gglasso <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
nlambda = 50
)
# Stop parallel processing
stopCluster(cl)
# Plot Group Lasso Fit
par(mfrow=c(1,1))
plot(fit_gglasso, main = "Group Lasso Coefficients Across Lambda")
### 4. Perform Cross-Validation for Group Lasso -----------------
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
set.seed(123)
cv_fit_gglasso <- cv.gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
nfolds = 10
)
stopCluster(cl)
# Optimal lambda
lambda_min_gl <- cv_fit_gglasso$lambda.min
cat("Optimal lambda (min):", lambda_min_gl, "\n")
# Optimal lambda
lambda_min_gl <- cv_fit_gglasso$lambda.min
lambda_1se_gl <- cv_fit_gglasso$lambda.1se
cat("Optimal lambda (min):", lambda_min_gl, "\n")
cat("Optimal lambda (1se):", lambda_1se_gl, "\n")
# Plot Cross-Validation Results for Group Lasso
plot(cv_fit_gglasso, main = "Cross-Validation for Group Lasso")
abline(v = log(lambda_min_gl), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se_gl), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
### 5. Fit the Final Group Lasso Model -------------------------
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
group_lasso_model <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
lambda = lambda_min_gl
)
stopCluster(cl)
### 6. Evaluate Group Lasso Model ------------------------------
# Predicted probabilities
log_odds_gl <- predict(group_lasso_model, newx = X_test_scaled, type = "link")
log_odds_gl
predicted_probs_gl <- 1 / (1 + exp(-log_odds_gl))
gl_metrics <- calculate_metrics(predicted_probs_gl, test_data$default_90)
gl_metrics
### 7. Post-Estimation Plots for Group Lasso -------------------
y_test_binary <- test_data$default_90
#### (1) ROC Curve and AUC-------------
roc_curve_gl <- roc(y_test_binary, predicted_probs_gl)
auc_value_gl <- auc(roc_curve_gl)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR side by side
plot(roc_curve_gl, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_gl, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### (2) Precision-Recall (PR) Curve -------------
# Generate PR curve
pr_curve_gl <- pr.curve(scores.class0 = predicted_probs_gl[y_test_binary == 1],
scores.class1 = predicted_probs_gl[y_test_binary == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve_gl, main = paste("Precision-Recall Curve (AUC =", round(pr_curve_gl$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
# This one is also shitty, try to fix via threshold
# Optimize threshold
threshold_results_gl <- find_optimal_threshold(predicted_probs_gl, true_labels, thresholds)
threshold_results_gl
plot_metrics(threshold_results_gl)
#### (2) Confusion Matrix Heatmap-----------
predicted_class_gl <- ifelse(predicted_probs_gl > 0.25, 1, -1)
conf_matrix_gl <- table(Predicted = predicted_class_gl, Actual = y_test)
ggplot(as.data.frame(conf_matrix_gl), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap (Group Lasso)", x = "Actual", y = "Predicted") +
theme_minimal()
#### (3) Coefficient Plot---------------
# Top N coefficients Group Lasso
# Convert coefficients to data frame and filter
coef_data_gl <- as.data.frame(as.matrix(coef(group_lasso_model)))
coef_data_gl$Variable <- rownames(coef_data_gl)
colnames(coef_data_gl) <- c("Coefficient", "Variable")
coef_data_gl <- coef_data_gl %>%
filter(Coefficient != 0 & Variable != "(Intercept)")
# Select top N coefficients by absolute value
top_n <- 25  # Adjust this number to show more/less coefficients
coef_data_gl <- coef_data_gl %>%
mutate(AbsCoefficient = abs(Coefficient)) %>%
arrange(desc(AbsCoefficient)) %>%
slice(1:top_n)
# Top N coefficients Group Lasso
# Convert coefficients to data frame and filter
coef_data_gl <- as.data.frame(as.matrix(coef(group_lasso_model)))
coef_data_gl$Variable <- rownames(coef_data_gl)
colnames(coef_data_gl) <- c("Coefficient", "Variable")
coef_data_gl <- coef_data_gl %>%
filter(Coefficient != 0 & Variable != "(Intercept)")
# Select top N coefficients by absolute value
top_n <- 25  # Adjust this number to show more/less coefficients
coef_data_gl <- coef_data_gl %>%
mutate(AbsCoefficient = abs(Coefficient)) %>%
arrange(desc(AbsCoefficient)) %>%
slice(1:top_n)
coef_data_gl <- coef_data_gl %>%
mutate(AbsCoefficient = abs(Coefficient)) %>%
arrange(desc(AbsCoefficient)) %>%
filter(row_number() <= top_n)
# Plot the top N coefficients
ggplot(coef_data_gl, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Group Lasso Top Coefficients Plot",
x = "Variable", y = "Coefficient") +
theme_minimal()
## GAM Model----------------
# we can use the natural variable selection that lasso provides:
print(vars_lasso)
# Using most relevant poly order (`s()` indicates the poly order for each predictor)
# GAM using Lasso selected vars
gam_model <- gam(default_90 ~ agency + income_group + s(contributions_balance) +
s(credit_limit)+
s(date_approval) + s(credit_duration) + s(dtf_approval_date)+
has_codebtor + m_date_limit,
data = train_data,
family = binomial)
summary(gam_model)
# re run model with no splines for pvalue <0.05
gam_model <- gam(default_90 ~ agency + income_group + s(contributions_balance) +
s(credit_limit)+
s(date_approval) + credit_duration + dtf_approval_date+
has_codebtor + m_date_limit,
data = train_data,
family = binomial)
summary(gam_model)
plot(gam_model, pages = 1, rug = TRUE)
# Summary of GAM Model
summary(gam_model)
### Model Evaluation-------------------
predicted_probs_gam <- predict(gam_model, newdata = test_data, type = "response")
gam_metrics <- calculate_metrics(predicted_probs_gam, test_data$default_90, 0.25)
gam_metrics
# True labels
y_test <- test_data$default_90
#### (1) ROC Curve and AUC-------------
roc_curve_gam <- roc(y_test, predicted_probs_gam)
auc_value_gam <- auc(roc_curve_gam)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR curves side by side
plot(roc_curve_gam, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_gam, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# Filter predicted probabilities based on the true labels
scores_class0 <- predicted_probs_gam[test_data$default_90 == 1]
scores_class1 <- predicted_probs_gam[test_data$default_90 == 0]
# Make sure the filtered vectors are numeric
scores_class0 <- as.numeric(scores_class0)
scores_class1 <- as.numeric(scores_class1)
pr_curve <- pr.curve(scores.class0 = scores_class0,
scores.class1 = scores_class1,
curve = TRUE)
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
# this model sucks, lets try adjusting threshold
### Fine Tunning Threshold------------
threshold_results_gam <- find_optimal_threshold(predicted_probs_gam, true_labels, thresholds)
threshold_results_gam
plot_metrics(threshold_results_gam)
#### 2. Confusion Matrix Heatmap -----------------------------
# Predicted classes
predicted_class <- ifelse(predicted_probs_gam > 0.25, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
# Set threshold to classify probabilities into binary outcomes
threshold <- 0.25
predicted_class <- ifelse(pred_probs_xgb > threshold, 1, 0)
### 1. Prepare data-------------
# Ensure the target variable is a factor
train_data$default_90 <- as.factor(train_data$default_90)
test_data$default_90 <- as.factor(test_data$default_90)
# Convert train and test datasets to numeric matrices
train_matrix <- model.matrix(default_90 ~ . - 1, data = train_data)
test_matrix <- model.matrix(default_90 ~ . - 1, data = test_data)
# Extract the labels
train_labels <- as.numeric(train_data$default_90) - 1  # Convert to 0/1
test_labels <- as.numeric(test_data$default_90) - 1    # Convert to 0/1
### 2. Train Model----------------
# Define XGBoost parameters
params <- list(
objective = "binary:logistic",  # Binary classification
eval_metric = "logloss",        # Evaluation metric
eta = 0.1,                      # Learning rate
max_depth = 6,                  # Maximum depth of trees
gamma = 0,                      # Minimum loss reduction
colsample_bytree = 0.8,         # Column subsample ratio
subsample = 0.8                 # Row subsample ratio
)
# Convert data to DMatrix format
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest <- xgb.DMatrix(data = test_matrix, label = test_labels)
# Train the XGBoost model
xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = 100,                  # Number of boosting rounds
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = 10,    # Early stopping
print_every_n = 10             # Print progress every 10 rounds
)
### 3. Evaluate Model-------------
# Predict probabilities on test data
pred_probs_xgb <- predict(xgb_model, newdata = dtest)
xgb_metrics <- calculate_metrics(pred_probs_xgb, test_data$default_90)
xgb_metrics
# Create confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
conf_matrix_df <- as.data.frame(as.table(conf_matrix))
# Plot heatmap
ggplot(conf_matrix_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(
title = "Confusion Matrix Heatmap",
x = "Actual",
y = "Predicted"
) +
theme_minimal()
### 3. Evaluate Model-------------
# Predict probabilities on test data
pred_probs_xgb <- predict(xgb_model, newdata = dtest)
xgb_metrics <- calculate_metrics(pred_probs_xgb, test_data$default_90)
xgb_metrics
threshold_results_xgb <- find_optimal_threshold(pred_probs_xgb,test_data$default_90, thresholds)
head(dtest)
dtest
threshold_results_xgb <- find_optimal_threshold(pred_probs_xgb,test_labels, thresholds)
xgb_metrics <- calculate_metrics(pred_probs_xgb, test_data$default_90)
xgb_metrics
xgb_metrics <- calculate_metrics(pred_probs_xgb, test_data$default_90, 0.25)
xgb_metrics
xgb_metrics <- calculate_metrics(pred_probs_xgb, test_data$default_90, 0.2)
xgb_metrics
xgb_metrics <- calculate_metrics(pred_probs_xgb, test_data$default_90, 0.15)
xgb_metrics
xgb_metrics <- calculate_metrics(pred_probs_xgb, test_data$default_90, 0.20)
xgb_metrics
threshold <- 0.20
predicted_class <- ifelse(pred_probs_xgb > threshold, 1, 0)
# Create confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
conf_matrix_df <- as.data.frame(as.table(conf_matrix))
# Plot heatmap
ggplot(conf_matrix_df, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(
title = "Confusion Matrix Heatmap",
x = "Actual",
y = "Predicted"
) +
theme_minimal()
xgb_metrics
#### Coefficients-------------------
install.packages("SHAPforxgboost")
install.packages("data.table")
library(SHAPforxgboost)
library(data.table)
# Convert your test dataset into a DMatrix
dtest <- xgb.DMatrix(data = test_matrix)
# Generate SHAP values for the test data
shap_values <- shap.values(xgb_model = xgb_model, X_train = train_matrix)
# Extract SHAP scores
shap_score <- shap_values$shap_score
# Plot a summary of SHAP values
shap_long <- shap.prep(xgb_model = xgb_model, X_train = train_matrix)
shap.plot.summary(shap_long)
##### Importance ---------------
xgb.importance(feature_names = colnames(train_matrix), model = xgb_model) %>%
xgb.plot.importance()
# Load Required Libraries --------------------------------
rm(list=ls())
library(readxl)
