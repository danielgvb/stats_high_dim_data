# Plot coefficients
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Coefficient Plot", x = "Variable", y = "Estimate") +
theme_minimal()
### 6.2.1 Train Model-------------------
# Start with an empty model
start_model <- glm(default_90 ~ 1, data = train_data_scaled, family = binomial)
# Define the full model with all predictors
full_model <- glm(default_90 ~ ., data = train_data_scaled, family = binomial)
# Perform forward selection
forward_model <- step(
start_model,
scope = list(lower = start_model, upper = full_model),
direction = "forward",
trace = 0
)
### 6.2.2 Model Evaluation-----------------------
summary(forward_model)
# Make predictions on the test data
predicted_probs_fw <- predict(forward_model, newdata = test_data_scaled, type = "response")
fw_metrics <- calculate_metrics(predicted_probs_fw, test_data_scaled$default_90)
fw_metrics
# True labels
true_labels <- test_data_scaled$default_90 # for model evaluation
roc_curve <- roc(true_labels, predicted_probs_fw)
auc_value <- auc(roc_curve)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR side by side
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### Precision-Recall (PR) Curve for Forward Model ------
# Generate PR curve
pr_curve <- pr.curve(scores.class0 = predicted_probs_fw[true_labels == 1],
scores.class1 = predicted_probs_fw[true_labels == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
#### Choose Threshold----------------------
thresholds_fw <- find_optimal_threshold(predicted_probs_fw, true_labels, thresholds)
thresholds_fw
plot_metrics(thresholds_fw)
# Results on optimal threshold
fw_metrics <- calculate_metrics(predicted_probs_fw, test_data_scaled$default_90, threshold = 0.5)
fw_metrics
#### Confusion Matrix Heatmap for Forward Model---------
predicted_class_fw <- ifelse(predicted_probs_fw > 0.5, 1, 0)
conf_matrix_fw <- table(Predicted = predicted_class_fw, Actual = test_data_scaled$default_90)
ggplot(as.data.frame(conf_matrix_fw), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap (Forward Model)", x = "Actual", y = "Predicted") +
theme_minimal()
#### 4. Coefficient Plot for Forward Model--------------
coef_data <- as.data.frame(summary(forward_model)$coefficients)
coef_data$Variable <- rownames(coef_data)
rownames(coef_data) <- NULL
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Coefficient Plot (Forward Model)", x = "Variable", y = "Estimate") +
theme_minimal()
## 6.3 Stepwise Backward Logit ---------------------
### 6.3.1 Train Model-------------------------
# Start with the full model
backward_model <- step(
full_model,
direction = "backward",
trace = 0
)
### 6.3.2 Model Evaluation------------------------------------
summary(backward_model)
# Make predictions on the test data
predicted_probs_bw <- predict(backward_model, newdata = test_data_scaled, type = "response")
bw_metrics <- calculate_metrics(predicted_probs_bw, test_data_scaled$default_90)
bw_metrics
# True labels
true_labels <- test_data_scaled$default_90
roc_curve <- roc(true_labels, predicted_probs_bw)
auc_value <- auc(roc_curve)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR side by side
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
##### Precision-Recall (PR) Curve for Backward Model ------
# Generate PR curve
pr_curve <- pr.curve(scores.class0 = predicted_probs_bw[true_labels == 1],
scores.class1 = predicted_probs_bw[true_labels == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
##### Choose Threshold----------------------
thresholds_bw <- find_optimal_threshold(predicted_probs_bw, true_labels, thresholds)
thresholds_bw
plot_metrics(thresholds_bw)
# Results on optimal threshold
bw_metrics <- calculate_metrics(predicted_probs_bw, test_data_scaled$default_90,0.5)
bw_metrics
##### Confusion Matrix Heatmap for Backward Model ---------
predicted_class_bw <- ifelse(predicted_probs_bw > 0.5, 1, 0)
conf_matrix_bw <- table(Predicted = predicted_class_bw, Actual = test_data_scaled$default_90)
ggplot(as.data.frame(conf_matrix_bw), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap (Backward Model)", x = "Actual", y = "Predicted") +
theme_minimal()
##### Coefficient Plot for Backward Model --------------
coef_data <- as.data.frame(summary(backward_model)$coefficients)
coef_data$Variable <- rownames(coef_data)
rownames(coef_data) <- NULL
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Coefficient Plot (Backward Model)", x = "Variable", y = "Estimate") +
theme_minimal()
# Define groups for numeric and factor variables
data_no_target <- data[, !colnames(data) %in% "default_90"]
numeric_data <- data_no_target[sapply(data_no_target, is.numeric)]
non_numeric_data <- data_no_target[, !sapply(data_no_target, is.numeric)]
group_vector <- c()  # Stores group IDs
dummy_list <- list()  # Stores dummy variables
group_id <- 1
# Numeric variables (each variable is its own group)
group_vector <- c(group_vector, seq(group_id, length.out = ncol(numeric_data)))
group_id <- max(group_vector) + 1
# Factor variables (each factor's dummies are a group)
for (col_name in colnames(non_numeric_data)) {
# Create dummy variables, dropping the first level
dummies <- as.matrix(
model.matrix(as.formula(paste("~", col_name)),
data = data_no_target)[, -1]
)
# Check if the resulting dummies matrix has at least one column
if (!is.null(ncol(dummies)) && ncol(dummies) > 0) {
# Store the dummies in the list
dummy_list[[col_name]] <- dummies
# Add group markers for the current set of dummy variables
group_vector <- c(group_vector, rep(group_id, ncol(dummies)))
# Increment the group ID for the next factor
group_id <- group_id + 1
} else {
# Handle cases where no dummy variables are created (e.g., single-level factors)
warning(paste("No dummy variables created for", col_name, "as it has only one level."))
}
}
# Split dummy data
dummy_data <- do.call(cbind, dummy_list)
dummy_data_train = subset(dummy_data, split)
dummy_data_test = subset(dummy_data, !split)
# Split and scale numeric data
numeric_data_train = subset(numeric_data, split)
numeric_data_test = subset(numeric_data, !split)
preproc <- preProcess(numeric_data_train, method = c("center", "scale"))
numeric_data_train_scaled <- predict(preproc, numeric_data_train)
numeric_data_test_scaled <- predict(preproc, numeric_data_test)
# Merge data and define target variable
X_train <- as.matrix(cbind(numeric_data_train_scaled, dummy_data_train))
X_test <- as.matrix(cbind(numeric_data_test_scaled, dummy_data_test))
y <- ifelse(data$default_90 == 0, -1, 1)  # Convert target {0,1} to {-1, 1}
y_train = subset(y, split)
y_test = subset(y, !split)
X_train <- as.matrix(cbind(numeric_data_train_scaled, dummy_data_train))
X_test <- as.matrix(cbind(numeric_data_test_scaled, dummy_data_test))
y <- ifelse(data$default_90 == 0, -1, 1)  # Convert target {0,1} to {-1, 1}
y_train = subset(y, split)
y_test = subset(y, !split)
# get only features for prediction step
test_data_features <- X_test %>% select(-default_90)
X_test
colnames(X_test)
"default_90" in colnames(X_test)
"default_90" %in% colnames(X_test)
# get only features for prediction step
y_train
X_train <- as.matrix(cbind(numeric_data_train_scaled, dummy_data_train))
X_test <- as.matrix(cbind(numeric_data_test_scaled, dummy_data_test))
y <- ifelse(data$default_90 == 0, 0, 1)  # Convert target {0,1} to {-1, 1}
y_train = subset(y, split)
y_test = subset(y, !split)
y <- ifelse(data$default_90 == 0, 0, 1)  # Convert target {0,1}
y_train = subset(y, split)
y_test = subset(y, !split)
y
y <- ifelse(data$default_90 == 0, "No", "Yes")  # Convert target {0,1}
y_train = subset(y, split)
y_test = subset(y, !split)
head(y_train)
tune_grid <- expand.grid(
C = c(1, 10),  # Values for cost
sigma = c(0.01, 0.1)  # Values for gamma
)
#### F1 model----------
# Register parallel backend
cl <- makeCluster(detectCores() - 1)  # Use all but one core
registerDoParallel(cl)
on.exit(stopCluster(cl))  # Ensure cleanup
ctrl <- trainControl(method = "cv", # choose your CV method
number = 2, # choose a bigger number once it runs / maybe 5
summaryFunction = prSummary, # TO TUNE ON F1 SCORE
classProbs = T,
verboseIter = T
#sampling = "smote" # is good for imbalanced dataset
)
X_train <- as.matrix(cbind(numeric_data_train_scaled, dummy_data_train))
X_test <- as.matrix(cbind(numeric_data_test_scaled, dummy_data_test))
# Adjust target variable levels to valid R names
y <- ifelse(data$default_90 == 0, "No", "Yes")  # Convert target {0,1} to {No, Yes}
y_train = subset(y, split)
y_test = subset(y, !split)
head(X_train)
X_train <- as.matrix(cbind(numeric_data_train_scaled, dummy_data_train))
X_test <- as.matrix(cbind(numeric_data_test_scaled, dummy_data_test))
# Adjust target variable levels to valid R names
y <- ifelse(data$default_90 == 0, "No", "Yes")  # Convert target {0,1} to {No, Yes}
y_train = subset(y, split)
y_test = subset(y, !split)
tune_grid <- expand.grid(
C = c(1, 10),  # Values for cost
sigma = c(0.01, 0.1)  # Values for gamma
)
#### F1 model----------
# Register parallel backend
cl <- makeCluster(detectCores() - 1)  # Use all but one core
registerDoParallel(cl)
on.exit(stopCluster(cl))  # Ensure cleanup
ctrl <- trainControl(method = "cv", # choose your CV method
number = 2, # choose a bigger number once it runs / maybe 5
summaryFunction = prSummary, # TO TUNE ON F1 SCORE
classProbs = T,
verboseIter = T
#sampling = "smote" # is good for imbalanced dataset
)
svm_model <- train(X_train, y_train,
method = "svmRadial",
metric  = "F",
trControl = ctrl,
tuneGrid = tune_grid
)
# svm_model <- train(default_90 ~., data = train_data_svm,
#                    method = "svmRadial",
#                    preProcess = c("center", "scale"),
#                    #tuneLength = 10,
#                    metric = "F", # The metric used for tuning is the F1 SCORE
#                    trControl = ctrl,
#                    tuneGrid = tune_grid     # Hyperparameter grid
# )
stopCluster(cl)
svm_model
# probabilities:
svm_probs <- predict(svm_model, newdata = test_data_features, type = "prob")
# probabilities:
svm_probs <- predict(svm_model, newdata = X_test, type = "prob")
head(svm_probs)
prob_class_yes <- svm_probs$Yes
head(prob_class_yes)
svm_metrics <- calculate_metrics(prob_class_yes, test_data$default_90, threshold = 0.5)
svm_metrics
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- test_data_svm$default_90  # Binary outcome
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- y_test  # Binary outcome
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- test_data$default_90  # Binary outcome
roc_curve_svm <- roc(true_labels, prob_class_yes)
auc_value_svm <- auc(roc_curve_svm)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Layout for side-by-side plots
plot(roc_curve_svm, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_svm, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### Precision-Recall (PR) Curve--------------
# Generate PR curve
pr_curve_svm <- pr.curve(scores.class0 = prob_class_yes[true_labels == "Yes"],
scores.class1 = prob_class_yes[true_labels == "No"],
curve = TRUE)
head(prob_class_yes)
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- test_data$default_90  # Binary outcome
head(true_labels)
roc_curve_svm <- roc(true_labels, prob_class_yes)
auc_value_svm <- auc(roc_curve_svm)
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- y_test  # Binary outcome
roc_curve_svm <- roc(true_labels, prob_class_yes)
auc_value_svm <- auc(roc_curve_svm)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Layout for side-by-side plots
plot(roc_curve_svm, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_svm, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### Precision-Recall (PR) Curve--------------
# Generate PR curve
pr_curve_svm <- pr.curve(scores.class0 = prob_class_yes[true_labels == "Yes"],
scores.class1 = prob_class_yes[true_labels == "No"],
curve = TRUE)
# Plot PR Curve
plot(pr_curve_svm, main = paste("Precision-Recall Curve (AUC =", round(pr_curve_svm$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
#### Choose Threshold---------------------
threshold_results_svm <- find_optimal_threshold(prob_class_yes, test_data_scaled$default_90, thresholds)
plot_metrics(threshold_results_svm)
threshold_results_svm
# Evaluate metrics
svm_metrics_threshold <- calculate_metrics(prob_class_yes, test_data_scaled$default_90, threshold = 0.25)
tune_grid <- expand.grid(
C = c(1, 10),  # Values for cost
sigma = c(0.01, 0.1)  # Values for gamma
)
#### F1 model----------
# Register parallel backend
cl <- makeCluster(detectCores() - 1)  # Use all but one core
registerDoParallel(cl)
on.exit(stopCluster(cl))  # Ensure cleanup
ctrl <- trainControl(method = "cv", # choose your CV method
number = 2, # choose a bigger number once it runs / maybe 5
summaryFunction = prSummary, # TO TUNE ON F1 SCORE
classProbs = T,
verboseIter = T,
sampling = "smote" #  for imbalanced dataset
)
svm_model <- train(X_train, y_train,
method = "svmRadial",
metric  = "F",
trControl = ctrl,
tuneGrid = tune_grid
)
# svm_model <- train(default_90 ~., data = train_data_svm,
#                    method = "svmRadial",
#                    preProcess = c("center", "scale"),
#                    #tuneLength = 10,
#                    metric = "F", # The metric used for tuning is the F1 SCORE
#                    trControl = ctrl,
#                    tuneGrid = tune_grid     # Hyperparameter grid
# )
stopCluster(cl)
svm_model
svm_model
# probabilities:
svm_probs <- predict(svm_model, newdata = X_test, type = "prob")
head(svm_probs)
prob_class_yes <- svm_probs$Yes
head(prob_class_yes)
svm_metrics <- calculate_metrics(prob_class_yes, test_data$default_90, threshold = 0.5)
svm_metrics
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- y_test  # Binary outcome
roc_curve_svm <- roc(true_labels, prob_class_yes)
auc_value_svm <- auc(roc_curve_svm)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Layout for side-by-side plots
plot(roc_curve_svm, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_svm, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### Precision-Recall (PR) Curve--------------
# Generate PR curve
pr_curve_svm <- pr.curve(scores.class0 = prob_class_yes[true_labels == "Yes"],
scores.class1 = prob_class_yes[true_labels == "No"],
curve = TRUE)
# Plot PR Curve
plot(pr_curve_svm, main = paste("Precision-Recall Curve (AUC =", round(pr_curve_svm$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
#### Choose Threshold---------------------
threshold_results_svm <- find_optimal_threshold(prob_class_yes, test_data_scaled$default_90, thresholds)
plot_metrics(threshold_results_svm)
threshold_results_svm
# Evaluate metrics
svm_metrics_threshold <- calculate_metrics(prob_class_yes, test_data_scaled$default_90, threshold = 0.05)
print(svm_metrics_threshold)
#### Confusion Matrix Heatmap-------------------
# Generate confusion matrix
svm_preds <- predict(svm_model, newdata = test_data_features, type = "raw")
conf_matrix_svm <- table(Predicted = svm_preds, Actual = true_labels)
#### Confusion Matrix Heatmap-------------------
# Generate confusion matrix
svm_preds <- predict(svm_model, newdata = X_test, type = "raw")
conf_matrix_svm <- table(Predicted = svm_preds, Actual = true_labels)
# Plot confusion matrix heatmap
ggplot(as.data.frame(conf_matrix_svm), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
### 9.2 Train Model-----------------
# hyperparameters grid
tune_grid <- expand.grid(
C = c(0.1, 1, 10, 100),  # Values for cost
sigma = c(0.01, 0.1, 1)  # Values for gamma
)
#tune_grid <- expand.grid(
#   C = c(1, 10),  # Values for cost
#   sigma = c(0.01, 0.1)  # Values for gamma
# )
#### F1 model----------
# Register parallel backend
cl <- makeCluster(detectCores() - 1)  # Use all but one core
registerDoParallel(cl)
on.exit(stopCluster(cl))  # Ensure cleanup
ctrl <- trainControl(method = "cv", # choose your CV method
number = 5, # choose a bigger number once it runs / maybe 5
summaryFunction = prSummary, # TO TUNE ON F1 SCORE
classProbs = T,
verboseIter = T,
sampling = "smote" #  for imbalanced dataset
)
svm_model <- train(X_train, y_train,
method = "svmRadial",
metric  = "F",
trControl = ctrl,
tuneGrid = tune_grid
)
svm_model
# probabilities:
svm_probs <- predict(svm_model, newdata = X_test, type = "prob")
head(svm_probs)
prob_class_yes <- svm_probs$Yes
head(prob_class_yes)
svm_metrics <- calculate_metrics(prob_class_yes, test_data$default_90, threshold = 0.5)
svm_metrics
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- y_test  # Binary outcome
roc_curve_svm <- roc(true_labels, prob_class_yes)
auc_value_svm <- auc(roc_curve_svm)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Layout for side-by-side plots
plot(roc_curve_svm, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_svm, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### Precision-Recall (PR) Curve--------------
# Generate PR curve
pr_curve_svm <- pr.curve(scores.class0 = prob_class_yes[true_labels == "Yes"],
scores.class1 = prob_class_yes[true_labels == "No"],
curve = TRUE)
# Plot PR Curve
plot(pr_curve_svm, main = paste("Precision-Recall Curve (AUC =", round(pr_curve_svm$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
#### Choose Threshold---------------------
threshold_results_svm <- find_optimal_threshold(prob_class_yes, test_data_scaled$default_90, thresholds)
plot_metrics(threshold_results_svm)
threshold_results_svm
# Evaluate metrics
svm_metrics_threshold <- calculate_metrics(prob_class_yes, test_data_scaled$default_90, threshold = 0.3)
print(svm_metrics_threshold)
#### Accuracy Model--------------
ctrl <- trainControl(method = "cv", # choose your CV method
number = 5, # choose a bigger number once it runs / maybe 5
classProbs = T,
verboseIter = T,
sampling = "smote" #  for imbalanced dataset
)
cl <- makeCluster(detectCores() - 1)  # Use all but one core
registerDoParallel(cl)
svm_model <- train(X_train, y_train,
method = "svmRadial",
trControl = ctrl,
tuneGrid = tune_grid
)
stopCluster(cl)
svm_model
svm_model
# probabilities:
svm_probs <- predict(svm_model, newdata = X_test, type = "prob")
head(svm_probs)
prob_class_yes <- svm_probs$Yes
head(prob_class_yes)
svm_metrics <- calculate_metrics(prob_class_yes, test_data$default_90, threshold = 0.5)
svm_metrics
#### ROC Curve and AUC-------------------
# Generate the ROC curve and calculate AUC
true_labels <- y_test  # Binary outcome
roc_curve_svm <- roc(true_labels, prob_class_yes)
auc_value_svm <- auc(roc_curve_svm)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Layout for side-by-side plots
plot(roc_curve_svm, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_svm, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### Precision-Recall (PR) Curve--------------
# Generate PR curve
pr_curve_svm <- pr.curve(scores.class0 = prob_class_yes[true_labels == "Yes"],
scores.class1 = prob_class_yes[true_labels == "No"],
curve = TRUE)
# Plot PR Curve
plot(pr_curve_svm, main = paste("Precision-Recall Curve (AUC =", round(pr_curve_svm$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
#### Choose Threshold---------------------
threshold_results_svm <- find_optimal_threshold(prob_class_yes, test_data_scaled$default_90, thresholds)
#### Choose Threshold---------------------
threshold_results_svm <- find_optimal_threshold(prob_class_yes, test_data_scaled$default_90, thresholds)
plot_metrics(threshold_results_svm)
threshold_results_svm
# Evaluate metrics
svm_metrics_threshold <- calculate_metrics(prob_class_yes, test_data_scaled$default_90, threshold = 0.3)
print(svm_metrics_threshold)
#### Confusion Matrix Heatmap-------------------
# Generate confusion matrix
svm_preds <- predict(svm_model, newdata = X_test, type = "raw")
conf_matrix_svm <- table(Predicted = svm_preds, Actual = true_labels)
# Plot confusion matrix heatmap
ggplot(as.data.frame(conf_matrix_svm), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
