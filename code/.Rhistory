)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Backward Model)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
## Lasso Logistic Regression--------------
### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
x_train <- as.matrix(train_data %>% select(-default_90))
### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
x_train <- as.matrix(train_data %>% select(-default_90))
### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
train_data
### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
as.matrix(train_data)
### 1. Prepare Data for glmnet --------------------------------
# Convert data to matrix form (required by glmnet)
x_train <- as.matrix(train_data %>% select(-default_90))
x_train <- as.matrix(select(train_data, -default_90))
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
y_train <- train_data$default_90
x_test <- as.matrix(test_data[, colnames(test_data) != "default_90"])
y_test <- test_data$default_90
# scale predictors
x_train <- scale(x_train)
x_test <- scale(x_test)
x_train
# scale predictors
x_train <- scale(x_train)
x_train
?glmnet
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
y_train <- train_data$default_90
x_test <- as.matrix(test_data[, colnames(test_data) != "default_90"])
y_test <- test_data$default_90
# scale predictors
x_train <- scale(x_train)
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
y_train <- train_data$default_90
x_test <- as.matrix(test_data[, colnames(test_data) != "default_90"])
y_test <- test_data$default_90
### 2. Perform Cross-Validation for Lasso Logistic Regression --
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial",
standarize = T,alpha = 1, nfolds = 10)
# Plot of Cross-Validation Results
# Optimal lambda
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
# Plot deviance against log(lambda)
par(mfrow= c(1,1))
plot(cv_lasso, main = "Cross-Validation for Lasso Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
# Optimal lambda
lambda_optimal <- cv_lasso$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")
### 3. Fit the Final Model with Optimal Lambda -----------------
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = lambda_optimal)
### 4. Evaluate Model ------------------------------------------
# Predicted probabilities
predicted_prob <- predict(lasso_model, s = lambda_optimal, newx = x_test, type = "response")
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
y_train <- train_data$default_90
x_test <- as.matrix(test_data[, colnames(test_data) != "default_90"])
y_test <- test_data$default_90
### 2. Perform Cross-Validation for Lasso Logistic Regression --
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial",
standarize = T,alpha = 1, nfolds = 10)
# Plot of Cross-Validation Results
# Optimal lambda
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
# Plot deviance against log(lambda)
par(mfrow= c(1,1))
plot(cv_lasso, main = "Cross-Validation for Lasso Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
# Optimal lambda
lambda_optimal <- cv_lasso$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")
### 3. Fit the Final Model with Optimal Lambda -----------------
lasso_model <- glmnet(x_train, y_train, family = "binomial",standarize = T , alpha = 1, lambda = lambda_optimal)
### 4. Evaluate Model ------------------------------------------
# Predicted probabilities
predicted_prob <- predict(lasso_model, s = lambda_optimal, newx = x_test, type = "response")
predicted_prob
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Model evaluation metrics
evaluate_lasso <- function(predicted_class, predicted_prob, y_test) {
accuracy <- mean(predicted_class == y_test)
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
auc_value <- auc(roc(y_test, predicted_prob))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}
lasso_results <- evaluate_lasso(predicted_class, predicted_prob, y_test)
print(lasso_results)
# (1) ROC Curve and AUC
roc_curve <- roc(y_test, predicted_prob)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(lasso_results$auc, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# (2) Confusion Matrix Heatmap
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
# (3) Coefficient Plot
coef_data <- as.data.frame(as.matrix(coef(lasso_model, s = lambda_optimal)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Lasso Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
# (4) Calibration Curve
calibration_data <- data.frame(
Predicted = as.vector(predicted_prob),
Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
### 2. Perform Cross-Validation for Elastic Net Logistic Regression --
# Elastic net uses `alpha` to mix lasso (alpha = 1) and ridge (alpha = 0)
set.seed(123)
cv_elastic_net <- cv.glmnet(
x_train, y_train,
standarize = T,
family = "binomial",
alpha = 0.5,        # Elastic net (mix of lasso and ridge)
nfolds = 10         # 10-fold cross-validation
)
# Plot of Cross-Validation Results
# Optimal lambdas
lambda_min <- cv_elastic_net$lambda.min
lambda_1se <- cv_elastic_net$lambda.1se
# Plot deviance against log(lambda)
par(mfrow = c(1, 1))
plot(cv_elastic_net, main = "Cross-Validation for Elastic Net Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
cat("Optimal lambda (min):", lambda_min, "\n")
cat("Optimal lambda (1se):", lambda_1se, "\n")
# We find the lambda to the top left, meaning it could be further left:
# Define a custom lambda sequence (smaller values)
lambda_grid <- 10^seq(-10, 2, length = 100)  # Adjust range to include smaller lambda values
# Perform Cross-Validation with custom lambda grid
set.seed(123)
# Perform Cross-Validation with custom lambda grid
set.seed(123)
cv_elastic_net <- cv.glmnet(
x_train, y_train,
family = "binomial",
standarize = T,
alpha = 0.5,
lambda = lambda_grid,
nfolds = 10
)
# Optimal lambdas
lambda_min <- cv_elastic_net$lambda.min
lambda_1se <- cv_elastic_net$lambda.1se
# Plot the updated deviance vs log(lambda) with the extended search range
plot(cv_elastic_net, main = "Extended Cross-Validation for Elastic Net Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
### 3. Fit the Final Model with Optimal Lambda -----------------
elastic_net_model <- glmnet(
x_train, y_train,
family = "binomial",
standarize = T,
alpha = 0.5,        # Elastic net
lambda = lambda_min # Use lambda.min for final model
)
### 4. Evaluate Model ------------------------------------------
# Predicted probabilities
predicted_prob <- predict(elastic_net_model, s = lambda_min, newx = x_test, type = "response")
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)
# Model evaluation metrics
evaluate_elastic_net <- function(predicted_class, predicted_prob, y_test) {
accuracy <- mean(predicted_class == y_test)
confusion <- confusionMatrix(factor(predicted_class), factor(y_test))
auc_value <- auc(roc(y_test, predicted_prob))
f1_score <- 2 * (confusion$byClass["Pos Pred Value"] * confusion$byClass["Sensitivity"]) /
(confusion$byClass["Pos Pred Value"] + confusion$byClass["Sensitivity"])
return(list(accuracy = accuracy, f1_score = f1_score, auc = auc_value))
}
elastic_net_results <- evaluate_elastic_net(predicted_class, predicted_prob, y_test)
print(elastic_net_results)
# (1) ROC Curve and AUC
roc_curve <- roc(y_test, predicted_prob)
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(elastic_net_results$auc, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# (2) Confusion Matrix Heatmap
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
# (3) Coefficient Plot
coef_data <- as.data.frame(as.matrix(coef(elastic_net_model, s = lambda_min)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Elastic Net Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
# (4) Calibration Curve
calibration_data <- data.frame(
Predicted = as.vector(predicted_prob),
Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
### 1. Prepare Data for Group Lasso -------------------------
# Define groups for numeric and factor variables
# Train Set
train_data_no_target <- train_data[, !colnames(train_data) %in% "default_90"]
numeric_data_train <- train_data_no_target[sapply(train_data_no_target, is.numeric)]
non_numeric_data_train <- train_data_no_target[, !sapply(train_data_no_target, is.numeric)]
group_vector_train <- c()  # Stores group IDs
dummy_list_train <- list()  # Stores dummy variables
group_id <- 1
# Numeric variables (each variable is its own group)
group_vector_train <- c(group_vector_train, rep(group_id, ncol(numeric_data_train)))
group_id <- group_id + 1
# Factor variables (each factor's dummies are a group)
for (col_name in colnames(non_numeric_data_train)) {
# Create dummy variables, dropping the first level
dummies <- model.matrix(as.formula(paste("~", col_name)), data = train_data_no_target)[, -1]
# Ensure dummies is treated as a matrix
dummies <- as.matrix(dummies)
# Check if the resulting dummies matrix has at least one column
if (!is.null(ncol(dummies)) && ncol(dummies) > 0) {
# Store the dummies in the list
dummy_list_train[[col_name]] <- dummies
# Add group markers for the current set of dummy variables
group_vector_train <- c(group_vector_train, rep(group_id, ncol(dummies)))
# Increment the group ID for the next factor
group_id <- group_id + 1
} else {
# Handle cases where no dummy variables are created (e.g., single-level factors)
warning(paste("No dummy variables created for", col_name, "as it has only one level."))
}
}
# Combine numeric and dummy data for the train set
dummy_data_train <- do.call(cbind, dummy_list_train)
combined_data_train <- cbind(numeric_data_train, dummy_data_train)
X_train <- as.matrix(combined_data_train)
y_train <- ifelse(train_data$default_90 == 1, 1, -1)  # Convert target to {-1, 1}
# Test Set
test_data_no_target <- test_data[, !colnames(test_data) %in% "default_90"]
numeric_data_test <- test_data_no_target[sapply(test_data_no_target, is.numeric)]
non_numeric_data_test <- test_data_no_target[, !sapply(test_data_no_target, is.numeric)]
group_vector_test <- c()
dummy_list_test <- list()
group_id <- 1
# Numeric variables
group_vector_test <- c(group_vector_test, rep(group_id, ncol(numeric_data_test)))
group_id <- group_id + 1
# Factor variables
for (col_name in colnames(non_numeric_data_test)) {
dummies <- model.matrix(as.formula(paste("~", col_name)), data = test_data_no_target)[, -1]
dummies <- as.matrix(dummies)
if (!is.null(ncol(dummies)) && ncol(dummies) > 0) {
dummy_list_test[[col_name]] <- dummies
group_vector_test <- c(group_vector_test, rep(group_id, ncol(dummies)))
group_id <- group_id + 1
} else {
warning(paste("No dummy variables created for", col_name, "as it has only one level."))
}
}
# Combine numeric and dummy data for the test set
dummy_data_test <- do.call(cbind, dummy_list_test)
combined_data_test <- cbind(numeric_data_test, dummy_data_test)
X_test <- as.matrix(combined_data_test)
y_test <- ifelse(test_data$default_90 == 1, 1, -1)  # Convert target to {-1, 1}
### 2. Standardize Predictors ---------------------------------
X_train
### 2. Standardize Predictors ---------------------------------
dim(X_train)
### 2. Standardize Predictors ---------------------------------
X_train_scaled <- scale(X_train)
X_test_scaled <- scale(X_test)
library(gglasso)
library(doParallel)
### 2. Standardize Predictors ---------------------------------
X_train_scaled <- scale(X_train)
X_test_scaled <- scale(X_test)
# Set up parallel processing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
# Fit group lasso model
fit_gglasso <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
nlambda = 50
)
# Stop parallel processing
stopCluster(cl)
# Plot Group Lasso Fit
plot(fit_gglasso, main = "Group Lasso Coefficients Across Lambda")
evaluate_gglasso_model <- function(model, test_x, test_y, threshold = 0.5) {
# Get the smallest lambda value
lambda_min <- min(model$lambda)
# Find the index of the smallest lambda
best_lambda_index <- which(model$lambda == lambda_min)
# Predict log-odds for the smallest lambda
log_odds <- predict(model, newx = test_x, type = "link")[, best_lambda_index]
# Convert log-odds to probabilities using the sigmoid function
predicted_prob <- 1 / (1 + exp(-log_odds))
# Convert probabilities to binary predictions using the threshold
predicted_class <- ifelse(predicted_prob > threshold, 1, -1)
# Ensure test_y is properly aligned
if (length(predicted_class) != length(test_y)) {
stop("Mismatch between predicted values and test_y. Check input dimensions.")
}
# Calculate accuracy
accuracy <- mean(predicted_class == test_y)
# Create confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_y)
# Precision, recall, and F1 score
true_positive <- ifelse("1" %in% rownames(conf_matrix) & "1" %in% colnames(conf_matrix),
conf_matrix["1", "1"], 0)
false_positive <- ifelse("1" %in% rownames(conf_matrix) & "-1" %in% colnames(conf_matrix),
conf_matrix["1", "-1"], 0)
false_negative <- ifelse("-1" %in% rownames(conf_matrix) & "1" %in% colnames(conf_matrix),
conf_matrix["-1", "1"], 0)
precision <- ifelse(true_positive + false_positive > 0,
true_positive / (true_positive + false_positive), 0)
recall <- ifelse(true_positive + false_negative > 0,
true_positive / (true_positive + false_negative), 0)
f1_score <- ifelse(precision + recall > 0,
2 * (precision * recall) / (precision + recall), 0)
# Return evaluation metrics
return(list(
accuracy = round(accuracy, 4),
f1_score = round(f1_score, 4),
confusion_matrix = conf_matrix
))
}
results_gglasso <- evaluate_gglasso_model(
model = fit_gglasso,
test_x = X_test_scaled,
test_y = y_test
)
# Print results
print(paste("Accuracy:", results_gglasso$accuracy))
print(paste("F1 Score:", results_gglasso$f1_score))
print("Confusion Matrix:")
print(results_gglasso$confusion_matrix)
### 5. Post-Estimation Plots -----------------------------------
# (1) Coefficient Plot for Optimal Lambda
coef_data <- as.data.frame(coef(fit_gglasso, s = lambda_optimal))
coef_data$Variable <- rownames(coef_data)
rownames(coef_data) <- NULL
coef_data <- coef_data %>% filter(s1 != 0 & Variable != "(Intercept)")
coefficients <- coef(fit_gglasso, s = lambda_optimal)
print(coefficients)
# Extract coefficients for the optimal lambda
lambda_optimal <- min(fit_gglasso$lambda)  # Smallest lambda
coefficients <- coef(fit_gglasso, s = lambda_optimal)
# Convert to a data frame
coef_data <- as.data.frame(as.matrix(coefficients))
coef_data$Variable <- rownames(coef_data)
rownames(coef_data) <- NULL
# Rename the coefficient column (default name is "s1" or similar)
colnames(coef_data)[1] <- "Coefficient"
# Filter for non-zero coefficients
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Group Lasso Coefficients", x = "Variable", y = "Coefficient") +
theme_minimal()
# (2) Calibration Curve
predicted_prob <- predict(fit_gglasso, newx = X_test_scaled, s = lambda_optimal, type = "response")
# Predict log-odds for the smallest lambda
lambda_min <- min(results_gglasso$lambda)
# Predict log-odds for the smallest lambda
lambda_min <- min(fit_gglasso$lambda)
# Find the index of the smallest lambda
best_lambda_index <- which(fit_gglasso$lambda == lambda_min)
log_odds <- predict(fit_gglasso, newx = X_test_scaled, type = "link")[, best_lambda_index]
# Convert log-odds to probabilities using the sigmoid function
predicted_prob <- 1 / (1 + exp(-log_odds))
calibration_data <- data.frame(
Predicted = as.vector(predicted_prob),
Observed = y_test
)
calibration_data$Bin <- cut(calibration_data$Predicted, breaks = 10, include.lowest = TRUE)
calibration_curve <- calibration_data %>%
group_by(Bin) %>%
summarize(
Mean_Predicted = mean(Predicted),
Mean_Observed = mean(Observed)
)
ggplot(calibration_curve, aes(x = Mean_Predicted, y = Mean_Observed)) +
geom_point() +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Calibration Curve (Group Lasso)", x = "Mean Predicted Probability", y = "Mean Observed Probability") +
theme_minimal()
## Group Lasso with cross validation----------------
# Set up k-fold cross-validation
k <- 10  # Number of folds
folds <- createFolds(y_train, k = k, list = TRUE)
# Prepare storage for cross-validation results
cv_results <- matrix(0, nrow = length(fit_gglasso$lambda), ncol = k)
# Set up parallel processing
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
# Perform cross-validation
for (fold_idx in 1:k) {
# Split data into training and validation sets for this fold
val_idx <- folds[[fold_idx]]
X_train_cv <- X_train_scaled[-val_idx, ]
y_train_cv <- y_train[-val_idx]
X_val_cv <- X_train_scaled[val_idx, ]
y_val_cv <- y_train[val_idx]
# Fit group lasso on training data
fit_cv <- gglasso(
x = X_train_cv,
y = y_train_cv,
group = group_vector_train,
loss = "logit",
nlambda = 50
)
# Evaluate on validation data
for (lambda_idx in 1:length(fit_cv$lambda)) {
# Predict for this lambda
log_odds <- predict(fit_cv, newx = X_val_cv, type = "link")[, lambda_idx]
predicted_prob <- 1 / (1 + exp(-log_odds))
predicted_class <- ifelse(predicted_prob > 0.5, 1, -1)
# Store validation accuracy
cv_results[lambda_idx, fold_idx] <- mean(predicted_class == y_val_cv)
}
}
# Stop parallel processing
stopCluster(cl)
# Average accuracy across folds for each lambda
cv_mean_accuracy <- rowMeans(cv_results)
# Identify the best lambda (maximum accuracy)
best_lambda_idx <- which.max(cv_mean_accuracy)
best_lambda <- fit_gglasso$lambda[best_lambda_idx]
# Print best lambda
cat("Best lambda:", best_lambda, "\n")
# Plot lambda min and 1se
plot(fit_gglasso)
# Refit the model on the full training data with the best lambda
final_fit <- gglasso(
x = X_train_scaled,
y = y_train,
group = group_vector_train,
loss = "logit",
lambda = best_lambda
)
# Evaluate the final model on the test set
log_odds_test <- predict(final_fit, newx = X_test_scaled, type = "link")
predicted_prob_test <- 1 / (1 + exp(-log_odds_test))
predicted_class_test <- ifelse(predicted_prob_test > 0.5, 1, -1)
accuracy_test <- mean(predicted_class_test == y_test)
cat("Test set accuracy with best lambda:", accuracy_test, "\n")
