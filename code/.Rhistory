}
for (col_name in colnames(numeric_data)) {
print(ggplot(train_data, aes(x = factor(default_90), y = .data[[col_name]], fill = factor(default_90))) +
geom_boxplot(alpha = 0.7) +
labs(title = paste("Boxplot of", col_name, "by Target"),
x = "Target",
y = col_name,
fill = "Target") +
theme_minimal())
}
### Density Plots--------------
for (col_name in colnames(numeric_data)) {
print(ggplot(train_data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +
labs(title = paste("Density Plot of", col_name), x = col_name, fill = "Target") +
theme_minimal())
}
### Non-Numeric Variables------------
non_numeric_data <- train_data[!sapply(train_data, is.numeric)]
unique_counts <- sapply(non_numeric_data, function(x) length(unique(x)))
mode_values <- sapply(non_numeric_data, function(x) names(which.max(table(x))))
print(mode_values)
cor_matrix <- cor(numeric_data, use = "complete.obs")
cor_matrix
melted_cor_matrix <- melt(cor_matrix)
ggplot(data = melted_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
theme_minimal() +
coord_fixed() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
### Barplots by Target-------
for (col_name in colnames(non_numeric_data)) {
print(
ggplot(train_data, aes_string(x = col_name, fill = "factor(default_90)")) +
geom_bar(position = "fill") +  # Use position = "dodge" for side-by-side bars
labs(title = paste("Bar Plot of", col_name, "by Target"),
x = col_name, y = "Proportion", fill = "Target") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
# Create a vector to store p-values
p_values <- c()
# Perform Chi-square tests and collect p-values
for (col_name in colnames(non_numeric_data)) {
contingency_table <- table(non_numeric_data[[col_name]], train_data$default_90)
chi2_result <- chisq.test(contingency_table)
# Store the p-value
p_values <- c(p_values, chi2_result$p.value)
}
# Create a data frame for plotting
chi2_results_df <- data.frame(
Variable = colnames(non_numeric_data),
P_value = p_values
)
# Plot the p-values
ggplot(chi2_results_df, aes(x = reorder(Variable, -P_value), y = P_value)) +
geom_bar(stat = "identity", fill = "skyblue") +
geom_hline(yintercept = 0.05, color = "red", linetype = "dashed") +  # Significance threshold
labs(title = "P-values from Chi-square Tests for Non-numeric Variables vs default_90",
x = "Variable",
y = "P-value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
#install.packages("woeBinning")
library(woeBinning)
# Binning for some vars
vars_to_binn <- c('rating', 'status', 'wd_date_approval', 'm_date_approval', 'work',
'max_education', 'city_born', 'agency')
# Initialize a list to store binning objects for each variable
binning_list <- list()
# Iterate over the variables to create binning for each
for (var in vars_to_binn) {
print(var)
binning_list[[var]] <- woeBinning::woe.binning(
df = train_data,
target.var = "default_90", # Target variable
pred.var = var,           # Current variable to bin
min.perc.total = 0.05,    # Minimum percentage per bin
min.perc.class = 0.01     # Minimum percentage of target class per bin
)
}
# Display binning summary for each variable
for (var in vars_to_binn) {
cat("\nBinning summary for", var, ":\n")
print(woeBinning::woe.binning.table(binning_list[[var]]))
}
# Apply binning transformations to the dataset
data_binned <- train_data
for (var in vars_to_binn) {
data_binned <- woeBinning::woe.binning.deploy(data_binned, binning_list[[var]])
}
# Iterate over the binned variables and generate plots
for (var in vars_to_binn) {
# Construct the binned variable name
binned_var <- paste0(var, ".binned")
# Check if the binned variable exists in the data
if (binned_var %in% names(data_binned)) {
# Generate and print the plot
plot <- ggplot(data_binned, aes_string(x = binned_var, fill = "factor(default_90)")) +
geom_bar(position = "fill") +  # Stacked proportions
labs(
title = paste("Bar Plot of", binned_var, "by Target"),
x = var,
y = "Proportion",
fill = "Target"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Print the plot
print(plot)
} else {
cat("Binned variable", binned_var, "does not exist in the data.\n")
}
}
# Apply binning to test data (under training rules)
test_data_binned <- test_data
for (var in vars_to_binn) {
# Check if the binning object exists and the variable is in test_data
if (!is.null(binning_list[[var]]) && var %in% names(test_data)) {
# Apply the binning to the test data
test_data_binned <- woeBinning::woe.binning.deploy(test_data_binned, binning_list[[var]])
} else {
cat("Skipping variable:", var, "as it is not found in test_data or binning_list.\n")
}
}
# Check the binned test data
head(test_data_binned)
# Drop original columns from train_data and test_data
train_data_binned <- data_binned[, !names(data_binned) %in% vars_to_binn]
test_data_binned <- test_data_binned[, !names(test_data_binned) %in% vars_to_binn]
# Model Training and Evaluation ------------------------
## Helper Functions------------------
# Define the function
calculate_metrics <- function(predicted_probs, actual_labels, threshold = 0.5) {
# Convert probabilities to binary predictions based on the threshold
predicted_labels <- ifelse(predicted_probs > threshold, 1, 0)
# Create confusion matrix
conf_matrix <- confusionMatrix(factor(predicted_labels), factor(actual_labels))
# Extract components of the confusion matrix
cm <- conf_matrix$table
TP <- cm[2, 2]  # True Positives
FP <- cm[1, 2]  # False Positives
TN <- cm[1, 1]  # True Negatives
FN <- cm[2, 1]  # False Negatives
# Manually calculate metrics
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1_score <- 2 * (precision * recall) / (precision + recall)
accuracy <- (TP + TN) / sum(cm)
# Return results as a list
return(list(
Precision = precision,
Recall = recall,
F1 = f1_score,
Accuracy = accuracy
))
}
# Function to find the optimal threshold
find_optimal_threshold <- function(predicted_probs, actual_labels, thresholds) {
results <- data.frame(Threshold = numeric(), Precision = numeric(),
Recall = numeric(), F1 = numeric(), Accuracy = numeric())
for (threshold in thresholds) {
# Safeguard against confusion matrices that don't return 4 cells
tryCatch({
metrics <- calculate_metrics(predicted_probs, actual_labels, threshold)
results <- rbind(results, c(Threshold = threshold, metrics))
}, error = function(e) {})
}
# Convert results to data frame
results <- as.data.frame(results)
# Filter for valid rows (non-NA F1 scores)
results <- results[!is.na(results$F1), ]
# Find the threshold that maximizes F1 score
optimal_threshold <- results$Threshold[which.max(results$F1)]
return(list(OptimalThreshold = optimal_threshold, Metrics = results))
}
plot_metrics <- function(metrics) {
# Reshape data for ggplot2
metrics_long <- reshape2::melt(metrics, id.vars = "Threshold",
variable.name = "Metric", value.name = "Value")
# Plot metrics
ggplot(metrics_long, aes(x = Threshold, y = Value, color = Metric)) +
geom_line() +
labs(title = "Metrics Across Thresholds", x = "Threshold", y = "Value") +
theme_minimal()
}
## Logistic Regression----------------
logistic_model <- glm(default_90 ~ ., data = train_data, family = binomial)
logistic_model_b <- glm(default_90 ~ ., data = train_data_binned, family = binomial)
# Make predictions on the test data
predicted_probs <- predict(logistic_model, newdata = test_data, type = "response")
# now check on binned data
predicted_probs_b <- predict(logistic_model_b, newdata = test_data_binned, type = "response")
### Model Evaluation---------------
logit_metrics <- calculate_metrics(predicted_probs, test_data$default_90)
logit_metrics
# metrics on binned data
logit_metrics_b <- calculate_metrics(predicted_probs_b, test_data_binned$default_90)
logit_metrics_b
### Post-Estimation Plots---------------
par(mfrow = c(2,2))
plot(logistic_model)
plot(logistic_model_b)
# reset grid
par(mfrow = c(1,1))
#### 1. ROC Curve and AUC ------------------------------------
# Predicted probabilities for the test data
predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")
# True labels
true_labels <- test_data$default_90
roc_curve <- roc(true_labels, predicted_prob)
auc_value <- auc(roc_curve)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set plot layout for ROC and PR curves side by side
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### 2. Precision-Recall (PR) Curve --------------------------
# Generate PR Curve
pr_curve <- pr.curve(scores.class0 = predicted_prob[true_labels == 1],
scores.class1 = predicted_prob[true_labels == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
# precision drops significantly, indicating that the model-
#- misclassifies many observations as positive when predicting defaults.
# Define a range of thresholds to evaluate
thresholds <- seq(0, 1, by = 0.05)  # Example grid of thresholds
thresholds_logit<- find_optimal_threshold(predicted_prob, true_labels, thresholds)
thresholds_logit
plot_metrics(thresholds_logit)
# for binned data fine tune threshold
thresholds_logit_b <- find_optimal_threshold(predicted_probs_b, true_labels, thresholds)
thresholds_logit_b
plot_metrics(thresholds_logit_b)
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.25, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
#### 4. Coefficient Plot -------------------------------------
# Extract coefficients
coefficients <- summary(logistic_model)$coefficients
thresholds_logit<- find_optimal_threshold(predicted_prob, true_labels, thresholds)
thresholds_logit
plot_metrics(thresholds_logit)
logit_metrics <- calculate_metrics(predicted_probs, test_data$default_90,0.25)
logit_metrics
logit_metrics <- calculate_metrics(predicted_probs, test_data$default_90,0.25)
logit_metrics
# Predicted classes
predicted_class <- ifelse(predicted_prob > 0.25, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
#### 4. Coefficient Plot -------------------------------------
# Extract coefficients
coefficients <- summary(logistic_model)$coefficients
coef_data <- as.data.frame(coefficients)
coef_data$Variable <- rownames(coefficients)
rownames(coef_data) <- NULL
# Plot coefficients
ggplot(coef_data, aes(x = reorder(Variable, Estimate), y = Estimate)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Coefficient Plot", x = "Variable", y = "Estimate") +
theme_minimal()
x_train <- as.matrix(train_data[, colnames(train_data) != "default_90"])
y_train <- train_data$default_90
x_test <- as.matrix(test_data[, colnames(test_data) != "default_90"])
y_test <- test_data$default_90
### 2. Perform Cross-Validation for Lasso Logistic Regression --
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial",
standarize = T,alpha = 1, nfolds = 10)
# Plot of Cross-Validation Results
# Optimal lambda
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
# Plot deviance against log(lambda)
par(mfrow= c(1,1))
plot(cv_lasso, main = "Cross-Validation for Lasso Logistic Regression")
abline(v = log(lambda_min), col = "blue", lty = 2, lwd = 2)  # Vertical line for lambda.min
abline(v = log(lambda_1se), col = "red", lty = 2, lwd = 2)   # Vertical line for lambda.1se
legend("topright", legend = c("lambda.min", "lambda.1se"), col = c("blue", "red"), lty = 2, lwd = 2)
# Optimal lambda
lambda_optimal <- cv_lasso$lambda.min
cat("Optimal lambda:", lambda_optimal, "\n")
### 3. Fit the Final Model with Optimal Lambda -----------------
lasso_model <- glmnet(x_train, y_train, family = "binomial", alpha = 1, lambda = lambda_optimal)
### Model Evaluation---------------
predicted_probs_l <- predict(lasso_model, s = lambda_optimal, newx = x_test, type = "response")
lasso_metrics <- calculate_metrics(predicted_probs_l, test_data$default_90, 0.3)
lasso_metrics
# True labels
true_labels <- y_test
#### 1. ROC Curve and AUC for Lasso Model----------
roc_curve <- roc(true_labels, predicted_probs_l)
auc_value <- auc(roc_curve)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR side by side
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### 2. Precision-Recall (PR) Curve for Lasso Model ------
# Generate PR curve
pr_curve <- pr.curve(scores.class0 = predicted_probs_l[true_labels == 1],
scores.class1 = predicted_probs_l[true_labels == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
# Define a range of thresholds to evaluate
thresholds <- seq(0, 1, by = 0.05)  # Example grid of thresholds
# Optimize threshold
threshold_results <- find_optimal_threshold(predicted_probs_l, true_labels, thresholds)
plot_metrics(threshold_results)
threshold_results
plot_metrics(threshold_results)
lasso_metrics <- calculate_metrics(predicted_probs_l, test_data$default_90, 0.15)
lasso_metrics
# True labels
true_labels <- y_test
#### 1. ROC Curve and AUC for Lasso Model----------
roc_curve <- roc(true_labels, predicted_probs_l)
auc_value <- auc(roc_curve)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR side by side
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
#### 2. Precision-Recall (PR) Curve for Lasso Model ------
# Generate PR curve
pr_curve <- pr.curve(scores.class0 = predicted_probs_l[true_labels == 1],
scores.class1 = predicted_probs_l[true_labels == 0],
curve = TRUE)
# Plot PR Curve
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
# Define a range of thresholds to evaluate
thresholds <- seq(0, 1, by = 0.05)  # Example grid of thresholds
# Optimize threshold
threshold_results <- find_optimal_threshold(predicted_probs_l, true_labels, thresholds)
threshold_results
plot_metrics(threshold_results)
#### (2) Confusion Matrix Heatmap------------
predicted_class <- ifelse(predicted_prob > 0.15, 1, 0) # using optimal threshold
conf_matrix <- table(Predicted = predicted_class, Actual = y_test)
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
#### (3) Coefficient Plot ----------------------------------------
coef_data <- as.data.frame(as.matrix(coef(lasso_model, s = lambda_optimal)))
coef_data$Variable <- rownames(coef_data)
colnames(coef_data) <- c("Coefficient", "Variable")
coef_data <- coef_data %>% filter(Coefficient != 0 & Variable != "(Intercept)")
ggplot(coef_data, aes(x = reorder(Variable, Coefficient), y = Coefficient)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs(title = "Lasso Coefficient Plot", x = "Variable", y = "Coefficient") +
theme_minimal()
# get lasso vars to use in GAM
vars_lasso <- coef_data$Variable
print(vars_lasso)
# Using most relevant poly order (`s()` indicates the poly order for each predictor)
# GAM using Lasso selected vars
gam_model <- gam(default_90 ~ agency + income_group + s(contributions_balance) +
s(credit_limit)+
s(date_approval) + s(credit_duration) + s(dtf_approval_date)+
has_codebtor + m_date_limit,
data = train_data,
family = binomial)
summary(gam_model)
# re run model with no splines for pvalue <0.05
gam_model <- gam(default_90 ~ agency + income_group + s(contributions_balance) +
s(credit_limit)+
s(date_approval) + credit_duration + dtf_approval_date+
has_codebtor + m_date_limit,
data = train_data,
family = binomial)
summary(gam_model)
plot(gam_model, pages = 1, rug = TRUE)
# Summary of GAM Model
summary(gam_model)
### Model Evaluation-------------------
predicted_probs_gam <- predict(gam_model, newdata = test_data, type = "response")
gam_metrics <- calculate_metrics(predicted_probs_gam, test_data$default_90, 0.3)
gam_metrics
# True labels
y_test <- test_data$default_90
#### (1) ROC Curve and AUC-------------
roc_curve_gam <- roc(y_test, predicted_probs_gam)
auc_value_gam <- auc(roc_curve_gam)
# Plot ROC Curve
par(mfrow = c(1, 2))  # Set layout to show ROC and PR curves side by side
plot(roc_curve_gam, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value_gam, 2), ")"))
abline(a = 0, b = 1, lty = 2, col = "gray")
# Filter predicted probabilities based on the true labels
scores_class0 <- predicted_probs_gam[test_data$default_90 == 1]
scores_class1 <- predicted_probs_gam[test_data$default_90 == 0]
# Make sure the filtered vectors are numeric
scores_class0 <- as.numeric(scores_class0)
scores_class1 <- as.numeric(scores_class1)
pr_curve <- pr.curve(scores.class0 = scores_class0,
scores.class1 = scores_class1,
curve = TRUE)
plot(pr_curve, main = paste("Precision-Recall Curve (AUC =", round(pr_curve$auc.integral, 2), ")"),
col = "red", xlab = "Recall", ylab = "Precision")
# this model sucks, lets try adjusting threshold
### Fine Tunning Threshold------------
threshold_results_gam <- find_optimal_threshold(predicted_probs_gam, true_labels, thresholds)
threshold_results_gam
gam_metrics <- calculate_metrics(predicted_probs_gam, test_data$default_90, 0.25)
gam_metrics
#### 2. Confusion Matrix Heatmap -----------------------------
# Predicted classes
predicted_class <- ifelse(predicted_probs_gam > 0.25, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = test_data$default_90)
# Heatmap
ggplot(as.data.frame(conf_matrix), aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "blue") +
geom_text(aes(label = Freq), color = "black") +
labs(title = "Confusion Matrix Heatmap", x = "Actual", y = "Predicted") +
theme_minimal()
# Convert the target variable to factor if not already
train_data$default_90 <- as.factor(train_data$default_90)
test_data$default_90 <- as.factor(test_data$default_90)
# Fit the Random Forest model
rf_model <- randomForest(
default_90 ~ .,       # Formula: target ~ predictors (all other columns)
data = train_data,    # Training dataset
ntree = 500,          # Number of trees (default is 500)
mtry = sqrt(ncol(train_data) - 1),  # Number of variables tried at each split
importance = TRUE,    # Measure variable importance
na.action = na.omit   # Handle missing values by omitting them
)
# View model summary
print(rf_model)
# Predict on test data
predictions <- predict(rf_model, newdata = test_data)
# Confusion Matrix
confusion_matrix <- table(Predicted = predictions, Actual = test_data$default_90)
print(confusion_matrix)
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", accuracy))
# Plot variable importance
importance(rf_model)       # Numerical importance
varImpPlot(rf_model)       # Plot importance
# Tune mtry
tune_rf <- tuneRF(
train_data[, -which(names(train_data) == "default_90")], # Exclude target variable
train_data$default_90,
stepFactor = 1.5,   # Increment of mtry
ntreeTry = 500,     # Number of trees to try
improve = 0.01,     # Minimum improvement in OOB error
trace = TRUE
)
# tunned results
tuned_mtry <- tune_rf[which.min(tune_rf[, 2]), 1]  # Extract mtry with minimum OOB error
print(paste("Optimal mtry:", tuned_mtry))
# tunned model
# Re-run the Random Forest model
rf_model_tuned <- randomForest(
default_90 ~ .,
data = train_data,
ntree = 500,        # Number of trees (can increase if needed)
mtry = tuned_mtry,  # Optimal mtry from tuning
importance = TRUE,
na.action = na.omit
)
# Print the tuned model summary
print(rf_model_tuned)
# Predict on test data
predictions_tuned <- predict(rf_model_tuned, newdata = test_data)
# Confusion Matrix
confusion_matrix_tuned <- table(Predicted = predictions_tuned, Actual = test_data$default_90)
print(confusion_matrix_tuned)
# Calculate accuracy
accuracy_tuned <- sum(diag(confusion_matrix_tuned)) / sum(confusion_matrix_tuned)
print(paste("Tuned Model Accuracy:", accuracy_tuned))
# metrics of tunned model
# Extract elements from the confusion matrix
TP <- confusion_matrix_tuned[2, 2]  # True Positives (default_90 = 1, Predicted = 1)
TN <- confusion_matrix_tuned[1, 1]  # True Negatives (default_90 = 0, Predicted = 0)
FP <- confusion_matrix_tuned[1, 2]  # False Positives (default_90 = 0, Predicted = 1)
FN <- confusion_matrix_tuned[2, 1]  # False Negatives (default_90 = 1, Predicted = 0)
# Calculate Recall
recall <- TP / (TP + FN)
print(paste("Recall:", recall))
# Calculate Precision
precision <- TP / (TP + FP)
print(paste("Precision:", precision))
# Calculate F1 Score
f1_score <- 2 * (precision * recall) / (precision + recall)
print(paste("F1 Score:", f1_score))
