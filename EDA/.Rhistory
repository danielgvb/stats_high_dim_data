geom_bar(stat = "identity", fill = "skyblue") +
geom_hline(yintercept = 0.05, color = "red", linetype = "dashed") +  # Significance threshold
labs(title = "P-values from Chi-square Tests for Non-numeric Variables vs default_90",
x = "Variable",
y = "P-value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Model----------------
## Logistic regression-------------------
# Basic logistic regression
model_base <- glm(default_90 ~ ., data = train_data, family = binomial)
plot(model_base)
summary(model_base)
# Glmnet
# Load necessary package
library(glmnet)
# Prepare training data matrix (assuming data is the training set)
train.x <- model.matrix(default_90 ~ . - 1, data = train_data)  # -1 removes the intercept column to avoid duplication
# Prepare target variable
train.y <- as.numeric(train_data$default_90)  # Ensure it's in numeric format (0 and 1) for glmnet
# Run the glmnet model
model1 <- glmnet(train.x, train.y, family = "binomial", standardize = T) # Data has different scales, so stand = True
plot(model1)
# run with cross validation
cv.model1 <- cv.glmnet(train.x, train.y, standardize = T, family = "binomial")
plot(cv.model1)
# See lambdas
cv.model1$lambda.min
cv.model1$lambda.1se
# measure = class
cv.model_c <- cv.glmnet(train.x, train.y, family = "binomial", type.measure = "class")
plot(cv.model_c)
# measure = AUC
cv.model_auc <- cv.glmnet(train.x, train.y, family = "binomial", type.measure = "auc")
plot(cv.model_auc)
# coefficients
coef.est <- as.matrix(coef(model1, s=cv.model1$lambda.min)) # change fit for model1
length(coef.est)
# coefficients different from 0
coef.vec <- subset(coef.est, coef.est != 0)
coef.vec
# Prepare training data matrix (assuming data is the training set)
test.x <- model.matrix(default_90 ~ . - 1, data = test_data)  # -1 removes the intercept column to avoid duplication
# Prepare target variable
test.y <- as.numeric(test_data$default_90)  # Ensure it's in numeric format (0 and 1) for glmnet
tpred.log <- predict(model1, test.x, s=cv.model1$lambda.min, type ="class") # type = "class" pred 0/1
str(tpred.log)
table(tpred.log, test.y)
err.log <- 1-sum(diag(table(tpred.log, test.y)))/NROW(test.y)
err.log
data <- read_excel("C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx")
# Import Data-------------
library(readxl)
data <- read_excel("C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx")
View(data)
# Remove ids-------
colnames(data)
# Some vars are ids, lets remove them to avoid perfect classification
data <- data[, !names(data) %in% c("Código de Crédito", "ID Cliente",
"No Pagaré Rotativo")]
str(data)
# Import Data-------------
library(readxl)
data <- read_excel("C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx")
# Remove ids-------
colnames(data)
# Some vars are ids, lets remove them to avoid perfect classification
data <- data[, !names(data) %in% c("Código de Crédito", "ID Cliente",
"No Pagaré Rotativo")]
str(data)
# Distinct values-------------
# Distinct values to see balance
# Count the number of distinct values for each column
num_distinct_values <- sapply(data, function(col) length(unique(col)))
# Print the number of distinct values for each column
num_distinct_values
#  "Clasificación Tipo Crédito" is single value, remove
data <- data[, !names(data) %in% c("Clasificación Tipo Crédito")]
# New user friendly names
colnames(data)
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity","credit_duration", "date_limit",
"default_90")
# Check that the number of new names matches the number of columns
if (length(friendly_names) == ncol(data)) {
# Assign the new names to the data frame
colnames(data) <- friendly_names
} else {
stop("The number of column names provided does not match the number of columns in the data frame.")
}
# Null Values--------
# Count of NA values in each column
na_counts <- colSums(is.na(data))
# Display columns with their respective NA counts
print(na_counts)
# Transform periodicity into numeric
library(dplyr)
# Mapping the values
data <- data %>%
mutate(periodicity_num = case_when(
periodicity == "Mensual" ~ 30,
periodicity == "Bimensual" ~ 60,
periodicity == "Quincenal" ~ 15,
TRUE ~ NA_real_  # Fallback for any unexpected values
))
View(data)
data <- data %>%
select(-periodicity)
View(data)
# Extract features from the date-time variables
# date approval
data$y_date_approval <- format(data$date_approval, "%Y")
data$m_date_approval <- format(data$date_approval, "%m")
data$d_date_approval <- format(data$date_approval, "%d")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$y_date_limit <- format(data$date_limit, "%Y")
data$m_date_limit <- format(data$date_limit, "%m")
data$d_date_limit <- format(data$date_limit, "%d")
data$wd_date_limit <- weekdays(data$date_limit)
# Convert POSIXct to numeric (number of seconds since 1970-01-01)
# as numeric
data$date_approval <- as.numeric(data$date_approval)
data$date_limit <- as.numeric(data$date_limit)
data <- read_excel("C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx")
# Remove ids-------
colnames(data)
# Some vars are ids, lets remove them to avoid perfect classification
data <- data[, !names(data) %in% c("Código de Crédito", "ID Cliente",
"No Pagaré Rotativo")]
str(data)
# Distinct values-------------
# Distinct values to see balance
# Count the number of distinct values for each column
num_distinct_values <- sapply(data, function(col) length(unique(col)))
# Print the number of distinct values for each column
num_distinct_values
#  "Clasificación Tipo Crédito" is single value, remove
data <- data[, !names(data) %in% c("Clasificación Tipo Crédito")]
# New user friendly names
colnames(data)
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity","credit_duration", "date_limit",
"default_90")
# Check that the number of new names matches the number of columns
if (length(friendly_names) == ncol(data)) {
# Assign the new names to the data frame
colnames(data) <- friendly_names
} else {
stop("The number of column names provided does not match the number of columns in the data frame.")
}
# Null Values--------
# Count of NA values in each column
na_counts <- colSums(is.na(data))
# Display columns with their respective NA counts
print(na_counts)
# Transform periodicity into numeric
library(dplyr)
# Mapping the values
data <- data %>%
mutate(periodicity_num = case_when(
periodicity == "Mensual" ~ 30,
periodicity == "Bimensual" ~ 60,
periodicity == "Quincenal" ~ 15,
TRUE ~ NA_real_  # Fallback for any unexpected values
))
data <- data %>%
select(-periodicity)
# Extract features from the date-time variables
# date approval
data$y_date_approval <- format(data$date_approval, "%Y")
View(data)
data$m_date_approval <- format(data$date_approval, "%m")
data$d_date_approval <- format(data$date_approval, "%d")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$y_date_limit <- format(data$date_limit, "%Y")
data$m_date_limit <- format(data$date_limit, "%m")
data$d_date_limit <- format(data$date_limit, "%d")
data$wd_date_limit <- weekdays(data$date_limit)
# Convert POSIXct to numeric (number of seconds since 1970-01-01)
# as numeric
data$date_approval <- as.numeric(data$date_approval)
data$date_limit <- as.numeric(data$date_limit)
View(data)
# Calculate the time difference between the two POSIXct variables (in days, hours, etc.)
data$time_difference_days <- as.numeric(difftime(data$date_limit, data$date_approval, units = "days"))
hist(data$time_difference_days)
mean(data$time_difference_days)
## Character to factor-------------
# Convert all character columns to factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
View(data)
# Train test split---------------------
#install.packages("caTools")
library(caTools)
# Set a seed for reproducibility
set.seed(123)
# Create a train-test split (70% train, 30% test)
split <- sample.split(data$default_90, SplitRatio = 0.7)
# Split the data into train and test sets
train_data <- subset(data, split == TRUE)      # Training set (70%)
test_data <- subset(data, split == FALSE)  # Test set (30%)
split
sample.split(60, SplitRatio = 0.1)
sample.split(c(1,2,3), SplitRatio = 0.1)
mean(train_data$default_90)
mean(test_data$default_90)
# Print the number of rows in each set to verify
cat("Number of rows in training set:", nrow(train_data), "\n")
cat("Number of rows in test set:", nrow(test_data), "\n")
View(data)
# EDA------------
library(skimr)
library(ggplot2)
library(reshape2)
## Numeric---------
numeric_data <- train_data[sapply(train_data, is.numeric)]
# Display the list of numeric column names
colnames(numeric_data)
# Variables Transformation-------------
# Create new variables
data$pct_payed <- data$contributions_balance / data$credit_limit
# Import Data-------------
library(readxl)
data <- read_excel("C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx")
# Remove ids-------
colnames(data)
# Some vars are ids, lets remove them to avoid perfect classification
data <- data[, !names(data) %in% c("Código de Crédito", "ID Cliente",
"No Pagaré Rotativo")]
str(data)
# Distinct values-------------
# Distinct values to see balance
# Count the number of distinct values for each column
num_distinct_values <- sapply(data, function(col) length(unique(col)))
# Print the number of distinct values for each column
num_distinct_values
#  "Clasificación Tipo Crédito" is single value, remove
data <- data[, !names(data) %in% c("Clasificación Tipo Crédito")]
# New user friendly names
colnames(data)
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity","credit_duration", "date_limit",
"default_90")
# Import Data-------------
library(readxl)
data <- read_excel("C:/Users/danie/Documents/GitHub/stats_high_dim_data/data/data.xlsx")
# Remove ids-------
colnames(data)
# Some vars are ids, lets remove them to avoid perfect classification
data <- data[, !names(data) %in% c("Código de Crédito", "ID Cliente",
"No Pagaré Rotativo")]
str(data)
# Distinct values-------------
# Distinct values to see balance
# Count the number of distinct values for each column
num_distinct_values <- sapply(data, function(col) length(unique(col)))
# Print the number of distinct values for each column
num_distinct_values
#  "Clasificación Tipo Crédito" is single value, remove
data <- data[, !names(data) %in% c("Clasificación Tipo Crédito")]
# New user friendly names
colnames(data)
friendly_names <- c("agency", "status", "rating", "work", "age", "civil_status",
"income_group", "city_born", "max_education", "gender",
"contributions_balance", "credit_limit", "capital_balance",
"capital_due30", "days_due", "date_approval",
"installment", "periodicity","credit_duration", "date_limit",
"default_90")
# Check that the number of new names matches the number of columns
if (length(friendly_names) == ncol(data)) {
# Assign the new names to the data frame
colnames(data) <- friendly_names
} else {
stop("The number of column names provided does not match the number of columns in the data frame.")
}
# Null Values--------
# Count of NA values in each column
na_counts <- colSums(is.na(data))
# Display columns with their respective NA counts
print(na_counts)
# Create new variables
# % of credit already payed
data$pct_payed <- data$contributions_balance / data$credit_limit
hist(data$pct_payed)
# Subset the data frame for credit limit above 50.000
subset_data <- data[data$credit_limit > 50000, ]
hist(subset_data$credit_limit, breaks = 30)
min(subset_data$credit_limit)
nrow(subset_data)
mean(subset_data$default_90)
# Assuming `data` is your data frame and `var` is the variable name as a string
library(ggplot2)
var <- "pct_payed"
# Create a CDF plot using ggplot2
ggplot(data, aes_string(x = var)) +
stat_ecdf(geom = "step") +
labs(title = paste("CDF Plot of", var),
x = var,
y = "Cumulative Probability") +
theme_minimal()
# Transform periodicity into numeric
library(dplyr)
# Mapping the values
data <- data %>%
mutate(periodicity_num = case_when(
periodicity == "Mensual" ~ 30,
periodicity == "Bimensual" ~ 60,
periodicity == "Quincenal" ~ 15,
TRUE ~ NA_real_  # Fallback for any unexpected values
))
# Mapp educ level
data <- data %>%
mutate(max_education = case_when(
max_education == "secundaria" ~ 2,
max_education == "técnico" ~ 3,
max_education == "tecnólogo" ~ 4,
max_education == "Universitario" ~ 5,
max_education == "Posgrado" ~ 6,
max_education == "primaria" ~ 1,
TRUE ~ NA_real_  # Fallback for any unexpected values
))
View(data)
data <- data %>%
select(-periodicity)
data$installment_periodic <- data$installment / data$periodicity_num
hist(data$installment_periodic)
# Extract features from the date-time variables
# date approval
data$y_date_approval <- format(data$date_approval, "%Y")
data$m_date_approval <- format(data$date_approval, "%m")
data$d_date_approval <- format(data$date_approval, "%d")
data$wd_date_approval <- weekdays(data$date_approval)
# date limit
data$y_date_limit <- format(data$date_limit, "%Y")
data$m_date_limit <- format(data$date_limit, "%m")
data$d_date_limit <- format(data$date_limit, "%d")
data$wd_date_limit <- weekdays(data$date_limit)
# Convert POSIXct to numeric (number of seconds since 1970-01-01)
# as numeric
data$date_approval <- as.numeric(data$date_approval)
data$date_limit <- as.numeric(data$date_limit)
# Calculate the time difference between the two POSIXct variables (in days, hours, etc.)
data$time_difference_days <- as.numeric(difftime(data$date_limit, data$date_approval, units = "days"))
hist(data$time_difference_days)
mean(data$time_difference_days)
## Character to factor-------------
# Convert all character columns to factors
data[] <- lapply(data, function(x) if (is.character(x)) as.factor(x) else x)
# Train test split---------------------
#install.packages("caTools")
library(caTools)
# Set a seed for reproducibility
set.seed(123)
# Create a train-test split (70% train, 30% test)
split <- sample.split(data$default_90, SplitRatio = 0.7)
# Split the data into train and test sets
train_data <- subset(data, split == TRUE)      # Training set (70%)
test_data <- subset(data, split == FALSE)  # Test set (30%)
# Print the number of rows in each set to verify
cat("Number of rows in training set:", nrow(train_data), "\n")
cat("Number of rows in test set:", nrow(test_data), "\n")
# EDA------------
library(skimr)
library(ggplot2)
library(reshape2)
## Numeric---------
numeric_data <- train_data[sapply(train_data, is.numeric)]
# Display the list of numeric column names
colnames(numeric_data)
# Describe basic statistics
skim(numeric_data)
### Histograms-------------
# Basic histogram
# Loop through each numeric column and create a histogram
for (col_name in colnames(numeric_data)) {
hist(numeric_data[[col_name]], main = paste("Histogram of", col_name),
xlab = col_name, col = "lightblue", border = "black")
}
### Correlation----------
# Correlation of Numeric Values:
# Calculate the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")
# Ugly heatmap
# Heatmap
heatmap(cor_matrix, Rowv = NA, Colv = NA)
# Melt the correlation matrix into a long format
melted_cor_matrix <- melt(cor_matrix)
# Set the factor levels for axes to maintain the order
melted_cor_matrix$Var1 <- factor(melted_cor_matrix$Var1, levels = colnames(cor_matrix))
melted_cor_matrix$Var2 <- factor(melted_cor_matrix$Var2, levels = colnames(cor_matrix))
# Create the heatmap
ggplot(data = melted_cor_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0,
limit = c(-1, 1), space = "Lab",
name = "Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
coord_fixed() +
labs(title = "Heatmap of Correlation Matrix",
x = "Variables",
y = "Variables")
for (col_name in colnames(numeric_data)) {
print(
ggplot(train_data, aes(x = factor(default_90), y = .data[[col_name]])) +
geom_boxplot(fill = "lightblue") +
labs(title = paste("Box Plot of", col_name, "by Target"),
x = "Target",
y = col_name) +
theme_minimal()
)
}
### Density plots---------
for (col_name in colnames(numeric_data)) {
print(
ggplot(train_data, aes(x = .data[[col_name]], fill = factor(default_90))) +
geom_density(alpha = 0.5) +
labs(title = paste("Densiy Plot of", col_name, "by Target"), x = col_name, fill = "Target") +
theme_minimal()
)
}
# Subset non-numeric columns
non_numeric_data <- train_data[sapply(train_data, Negate(is.numeric))]
# unique values
unique_counts <- sapply(non_numeric_data, function(x) length(unique(x)))
print(unique_counts)
# Mode analysis
# Function to find the mode
find_mode <- function(x) {
ux <- unique(x)
ux[which.max(tabulate(match(x, ux)))]
}
# Apply the function to each non-numeric column
mode_values <- sapply(non_numeric_data, find_mode)
print(mode_values)
# Loop through each non-numeric column and create a bar plot
for (col_name in colnames(non_numeric_data)) {
print(
ggplot(train_data, aes_string(x = col_name)) +
geom_bar(fill = "lightblue", color = "black") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = paste("Bar Plot of", col_name),
x = col_name,
y = "Count") +
scale_y_continuous(expand = c(0, 0))
)
}
for (col_name in colnames(non_numeric_data)) {
print(
ggplot(train_data, aes_string(x = col_name, fill = "factor(default_90)")) +
geom_bar(position = "fill") +  # Use position = "dodge" for side-by-side bars
labs(title = paste("Bar Plot of", col_name, "by Target"),
x = col_name, y = "Proportion", fill = "Target") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
)
}
# Create a vector to store p-values
p_values <- c()
# Perform Chi-square tests and collect p-values
for (col_name in colnames(non_numeric_data)) {
contingency_table <- table(non_numeric_data[[col_name]], train_data$default_90)
chi2_result <- chisq.test(contingency_table)
# Store the p-value
p_values <- c(p_values, chi2_result$p.value)
}
# Create a data frame for plotting
chi2_results_df <- data.frame(
Variable = colnames(non_numeric_data),
P_value = p_values
)
# Plot the p-values
ggplot(chi2_results_df, aes(x = reorder(Variable, -P_value), y = P_value)) +
geom_bar(stat = "identity", fill = "skyblue") +
geom_hline(yintercept = 0.05, color = "red", linetype = "dashed") +  # Significance threshold
labs(title = "P-values from Chi-square Tests for Non-numeric Variables vs default_90",
x = "Variable",
y = "P-value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Model----------------
## Logistic regression-------------------
# Basic logistic regression
model_base <- glm(default_90 ~ ., data = train_data, family = binomial)
plot(model_base)
summary(model_base)
# Glmnet
# Load necessary package
library(glmnet)
# Prepare training data matrix (assuming data is the training set)
train.x <- model.matrix(default_90 ~ . - 1, data = train_data)  # -1 removes the intercept column to avoid duplication
# Prepare target variable
train.y <- as.numeric(train_data$default_90)  # Ensure it's in numeric format (0 and 1) for glmnet
# Prepare training data matrix (assuming data is the training set)
test.x <- model.matrix(default_90 ~ . - 1, data = test_data)  # -1 removes the intercept column to avoid duplication
# Prepare target variable
test.y <- as.numeric(test_data$default_90)  # Ensure it's in numeric format (0 and 1) for glmnet
# Run the glmnet model
model1 <- glmnet(train.x, train.y, family = "binomial", standardize = T) # Data has different scales, so stand = True
plot(model1)
